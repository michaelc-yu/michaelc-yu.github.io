<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Math for ML | Michael Yu </title> <meta name="author" content="Michael Yu"> <meta name="description" content="notes from Mathematics for Machine Learning by Deisenroth, Faisal, Ong"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://michaelc-yu.github.io/blog/2024/math/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Michael</span> Yu </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/notes/index.html">notes </a> </li> <li class="nav-item "> <a class="nav-link" href="/research/">research </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Math for ML</h1> <p class="post-meta"> Created in November 24, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/mathematics"> <i class="fa-solid fa-hashtag fa-sm"></i> mathematics</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h4 id="vector-calculus">Vector Calculus</h4> <p>Taylor Polynomial is an approximation of a function f(x) around a point \(x_0\) using a polynomial constructed from the derivatives of f(x) at that point.</p> <p>Def: The Taylor Polynomial of degree n of \(f: \mathbb{R} \rightarrow \mathbb{R}\) at \(x_0\) is defined as <br> \(T_n(x) = \sum_{k=0}^n \frac{f^{(k)}(x_0)}{k!} (x-x_0)^k\) <br><br> where \(f^{(k)}(x_0)\) is the \(k^{\text{th}}\) derivative of f at \(x_0\) and \(\frac{f^{(k)}(x_0)}{k!}\) are the coefficients of the polynomial.</p> <p><br></p> <p><strong>Functions</strong> <br> Product rule: \((f(x) g(x))' = f'(x) g(x) + f(x) g'(x)\) <br> Quotient rule: \((f(x) / g(x))' = (f'(x) g(x) - f(x) g'(x)) / (g(x))^2\) <br> Sum rule: \((f(x) + g(x))' = f'(x) + g'(x)\) <br> Chain rule: \((g(f(x)))' = (g o f)'(x) = g'(f(x))f'(x)\)</p> <p><br> The generalization of the derivative to functions of several variables is the gradient. We find the gradient of the function \(f\) with respect to x by varying one variable at a time and keeping the others constant.</p> <p>Def: For a function \(f: \mathbb{R}^n \rightarrow \mathbb{R}, x \rightarrow f(x), x \in \mathbb{R}^n\) of n variables \(x_1, ..., x_n\) we define the partial derivatives as</p> <p>\(\frac{df}{dx_1}\) = \(\lim_{h \to 0} \frac{f(x_1 + h, x_2, ..., x_n) - f(x)}{h}\) <br> … <br> \(\frac{df}{dx_n}\) = \(\lim_{h \to 0} \frac{f(x_1, x_2, ..., x_n + h) - f(x)}{h}\)</p> <p>When we collect them in a row vector form, this is called the gradient of \(f\) or the <strong>Jacobian</strong>.</p> <p><br> Sometimes we are interested in derivatives of higher order. Consider a function \(f: \mathbb{R}^2 \rightarrow \mathbb{R}\) of two variables x, y. We use the following notation for higher-order partial derivatives (and for gradients):</p> <p>\(\frac{d^2 f}{dx^2}\) is the second partial derivative of f with respect to x <br> \(\frac{d^n f}{dx^n}\) is the \(n^{\text{th}}\) partial derivative of f with respect to x <br></p> <p>The Hessian is the collection of all second-order partial derivatives. It generalizes the second derivative to multiple dimensions and is used to study the curvature of a function. It is a square matrix.</p> <p><br></p> <p><strong>Multivariate Taylor Series</strong></p> <p>We consider a function \(f: \mathbb{R}^D \rightarrow \mathbb{R}\) \(x \rightarrow f(x), x \in \mathbb{R}^D\)</p> <p>that is smooth at \(x_0\). The multivariate Taylor series of \(f\) at \((x_0)\) is defined as</p> <p>\(f(x) = \sum_{k=0}^\infty \frac{D_x^{k}f(x_0)}{k!} (x-x_0)^k\) <br><br> where \({D_x^{k}f(x_0)}\) is the \(k^{\text{th}}\) total derivative of \(f\) with respect to x, evaluated at \(x_0\). <br><br> The Taylor series is an infinite series that represents a function as a sum of terms based on its derivatives at a point. The Taylor series provides a precise representation of the function if it converges to the function at all points near the expansion point. <br> The Taylor polynomial is a finite approximation of the Taylor series, using only the first few terms (up to n). A Taylor polynomial of degree n includes terms up to the \(n^{\text{th}}\) partial derivatives of the function. It provides a local approximation near a given point but becomes less accurate as you move further from that point.</p> <p><br></p> <h4 id="probability-and-distributions">Probability and Distributions</h4> <p>The sample space Ω is the set of all possible outcomes of the experiment.</p> <p>The event space \(A\) is a subset of the sample space that represents a particular outcome or group ot outcomes we’re interested in.</p> <p>The probability \(P\). With each event, we associate a number \(P(A)\) that measures the probability that the event will occur.</p> <p><br></p> <p>The marginal probabbility that \(X\) takes the value x irrespective of the value of random variable \(Y\) is written as p(x).</p> <p>The fraction of instances (the conditional probability) for which \(Y\) = y is written as \(p(y \mid x)\).</p> <p>Def: The <strong>product rule</strong> relates the joint distribution to the conditional distribution via p(x, y) = \(p(y \mid x)\) p(x)</p> <p><strong>Bayes’ theorem</strong> <br> \(p(x \mid y)\) = \(\frac {p(y \mid x) p(x)}{p(y)}\)</p> <p><br></p> <p><strong>Expectation</strong></p> <p>\(\mathbb{E}[X] = \sum_{i} x_i P(X=x_i)\) (for discrete variables)</p> <p>\(\mathbb{E}[X] = \int_{-\infty}^\infty x f_x(x) dx\) (for continuous variables)</p> <p>The expected value of a function g of a discrete random variable x is given by</p> \[\mathbb{E}[g(x)] = \sum_{x \in X} g(x) p(x)\] <p>The expected value of a function \(g: \mathbb{R} \rightarrow \mathbb{R}\) of a univariate continuous random variable x is given by</p> \[\mathbb{E}[g(x)] = \int_{x} g(x) p(x) dx\] <p><strong>Sums and Transformations of Random Variables</strong></p> \[\mathbb{E}[x+y] = \mathbb{E}[x] + \mathbb{E}[y]\] \[\mathbb{E}[x-y] = \mathbb{E}[x] - \mathbb{E}[y]\] <p><strong>Covariance</strong></p> <p>The covariance intuitively represents the notion of how dependent random variables are to one another.</p> <p>The covariance bewteen two univariate random variables X, Y \(\in \mathbb{R}\) is given by the expected product of their derivatives from their respective means,</p> \[Cov_{\text{x,y}} [x, y] = \mathbb{E}_{\text{x,y}} [(x-\mathbb{E}_x[x]) (y-\mathbb{E}_y[y])]\] <p>The covariance of a variable with itself is called the variance and is denoted by \(V_x[x]\). The square root of the variance is called the standard deviation \(\sigma(x)\)</p> <p>If we consider two multivariate random variables X and Y with states \(x \in \mathbb{R}^D\) and \(y \in \mathbb{R}^E\) respectively, the covariance between X and Y is defined as</p> <p>Cov[x, y] = \(\mathbb{E}[xy^T] - \mathbb{E}[x]\mathbb{E}[y]^T = Cov[y, x]^T \in \mathbb{R}^{\text{D x E}}\)</p> <p><strong>Correlation</strong></p> <p>The correlation between two random variables X, Y is given by</p> \[Corr[x, y] = \frac{Cov[x,y]}{\sqrt{V[x] V[y]}} \in [-1, 1]\] <p>The covariance (and correlation) indicate how two random variables are related. Positive correlation means that when x grows, then y is also expected to grow. Negative correlation means that as x increases, then y decreases.</p> <p><strong>Statistical Independence</strong></p> <p>Two random variables X,Y are statistically independent if and only if p(x,y) = p(x) p(y)</p> <p><strong>Gaussian Distribution</strong></p> <p>The Gaussian distribution is one of the most well-studied probability distributions for continuous-valued random variables. It’s also known as the normal distribution. For a univariate random variable, the Gaussian distribution has a density that is given by:</p> \[p(x \mid \mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x - \mu)^2}{2\sigma^2}}\] <h4 id="continuous-optimization">Continuous Optimization</h4> <p><strong>Gradient Descent</strong></p> <p>If we want to find a local optimum \(f(x_0)\) of a function \(f: \mathbb{R}^n \rightarrow \mathbb{R}, x \rightarrow f(x)\), we start with an initial guess \(x_0\) of the parameters we wish to optimize and then iterate according to</p> \[x_{\text{i+1}} = x_i - y_i ((\nabla{f}) (x_i))^T\] <p>For a suitable step-size \(y_i\), the sequence \(f(x_0) \geq f(x_1) \geq ...\) converges to a local minimum.</p> <p><strong>Convex Optimization</strong></p> <p>A <strong>convex function</strong> is a type of function with a specific shape and property that is crucial in optimization. Intuitively, a convex function has a bowl-shaped or U-shaped curve, which makes it easier to find the minimum of the function because any local minimum is also a global minimum.</p> <p>A set C is a <strong>convex set</strong> if for any \(x, y \in C\) and for any scalar \(\theta\) with \(0 \leq \theta \leq 1\), we have</p> \[\theta x + (1 - \theta) y \in C\] <p>Convex sets are sets such that a straight line connecting any two elements of the set lie inside the set.</p> <p><strong>Convex functions</strong> are functions such that a straight line between any two points of the function lie above the function.</p> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Michael Yu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> </body> </html>