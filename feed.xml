<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://michaelc-yu.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://michaelc-yu.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-10-30T04:40:53+00:00</updated><id>https://michaelc-yu.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">neuroscience notes</title><link href="https://michaelc-yu.github.io/blog/2024/neuroscience/" rel="alternate" type="text/html" title="neuroscience notes"/><published>2024-10-26T23:36:10+00:00</published><updated>2024-10-26T23:36:10+00:00</updated><id>https://michaelc-yu.github.io/blog/2024/neuroscience</id><content type="html" xml:base="https://michaelc-yu.github.io/blog/2024/neuroscience/"><![CDATA[]]></content><author><name></name></author><category term="neuroscience"/><summary type="html"><![CDATA[interesting concepts from neuroscience and psychobiology]]></summary></entry><entry><title type="html">LLM optimizations</title><link href="https://michaelc-yu.github.io/blog/2024/optimizations/" rel="alternate" type="text/html" title="LLM optimizations"/><published>2024-10-26T23:36:10+00:00</published><updated>2024-10-26T23:36:10+00:00</updated><id>https://michaelc-yu.github.io/blog/2024/optimizations</id><content type="html" xml:base="https://michaelc-yu.github.io/blog/2024/optimizations/"><![CDATA[<h2 id="optimizing-the-attention-mechanism">Optimizing the attention mechanism</h2> <h3 id="multi-query-attention">Multi-query attention</h3> <p>Multi-query attention (MQA) shares the keys and values among all the attention heads. The query vector is still projected multiple times, as before, but there is one set of keys and values for all heads. While the amount of computation done in MQA is identical to multi-head attention, the amount of data (keys, values) read from memory is a fraction of before. When bound by memory-bandwidth, this enables better compute utilization. It also reduces the size of the KV-cache in memory, allowing space for larger batch sizes.</p> <h3 id="grouped-query-attention">Grouped-query attention</h3> <p>Grouped-query attention (GQA) projects keys and values to a few groups of query heads. It has more key-value heads than one, but fewer than the number of query heads. It’s a balance between multi-head attention and multi-query attention.</p> <figure> <picture> <img src="/assets/img/gqa.jpg" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="flash-attention">Flash attention</h3> <p>Another way of optimizing the attention mechanism is to modify the ordering of certain computations to take better advantage of the memory hierarchy of GPUs. Neural networks are generally described in terms of layers, and most implementations are laid out that way as well, with one kind of computation done on the input data at a time in sequence. This doesn’t always lead to optimal performance, since it can be beneficial to do more calculations on values that have already been brought into the higher, more performant levels of the memory hierarchy.</p> <p>Fusing multiple layers together during the actual computation can enable minimizing the number of times the GPU needs to read from and write to its memory and to group together calculations that require the same data, even if they are parts of different layers in the neural network.</p> <p>One very popular fusion is FlashAttention, an I/O aware exact attention algorithm, as detailed in FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness. Exact attention means that it is mathematically identical to the standard multi-head attention (with variants available for multi-query and grouped-query attention), and so can be swapped into an existing model architecture or even an already-trained model with no modifications.</p> <p>I/O aware means it takes into account some of the memory movement costs previously discussed when fusing operations together. In particular, FlashAttention using “tiling” to fully compute and write out a small part of the final matrix at once, rather than doing part of the computation on the whole matrix in steps, writing out the intermediate values in between.</p> <figure> <picture> <img src="/assets/img/flashattn.jpg" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <hr/> <h2 id="modifications-to-model-weights">Modifications to model weights</h2> <h3 id="quantization">Quantization</h3> <p>Quantization is the process of reducing the precision of a model’s weights and activations. Most models are trained with 32 or 16 bits of precision, where each parameter and activation element takes up 32 or 16 bits of memory - a single-precision floating point. However, most deep learning models can be effectively represented with eight or even fewer bits per value.</p> <h3 id="sparsity">Sparsity</h3> <p>Similar to quantization, it’s been shown that many deep learning models are robust to pruning, or replacing certain values that are close to 0 with 0 itself. Sparse matrices are matrices where many of the elements are 0. These can be expressed in a condensed form that takes up less space than a full, dense matrix.</p> <h3 id="distillation">Distillation</h3> <p>This process involves training a smaller model that’s called a student model to mimic the behavior of the larger model (the teacher model). The student model will be trained to mirror the performance of the teacher model, with a loss function that measures the discrepancy between their outputs. DistilBERT compresses a BERT model by 40% while retaining 97% of its language understanding capabilities at a 60% faster speed.</p>]]></content><author><name></name></author><category term="machine-learning"/><summary type="html"><![CDATA[walkthrough of various model optimization techniques]]></summary></entry><entry><title type="html">transformer architecture</title><link href="https://michaelc-yu.github.io/blog/2024/transformer/" rel="alternate" type="text/html" title="transformer architecture"/><published>2024-10-26T15:09:00+00:00</published><updated>2024-10-26T15:09:00+00:00</updated><id>https://michaelc-yu.github.io/blog/2024/transformer</id><content type="html" xml:base="https://michaelc-yu.github.io/blog/2024/transformer/"><![CDATA[<p>Work in progress</p>]]></content><author><name></name></author><category term="machine-learning"/><summary type="html"><![CDATA[in-depth walkthrough of transformer architecture]]></summary></entry></feed>