<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://michaelc-yu.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://michaelc-yu.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-11-24T22:18:03+00:00</updated><id>https://michaelc-yu.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">math formulas</title><link href="https://michaelc-yu.github.io/blog/2024/math/" rel="alternate" type="text/html" title="math formulas"/><published>2024-11-24T13:36:11+00:00</published><updated>2024-11-24T13:36:11+00:00</updated><id>https://michaelc-yu.github.io/blog/2024/math</id><content type="html" xml:base="https://michaelc-yu.github.io/blog/2024/math/"><![CDATA[<h4 id="vector-calculus">Vector Calculus</h4> <p>Taylor Polynomial is an approximation of a function f(x) around a point \(x_0\) using a polynomial constructed from the derivatives of f(x) at that point.</p> <p>Def: The Taylor Polynomial of degree n of \(f: \mathbb{R} \rightarrow \mathbb{R}\) at \(x_0\) is defined as <br/> \(T_n(x) = \sum_{k=0}^n \frac{f^{(k)}(x_0)}{k!} (x-x_0)^k\) <br/><br/> where \(f^{(k)}(x_0)\) is the \(k^{\text{th}}\) derivative of f at \(x_0\) and \(\frac{f^{(k)}(x_0)}{k!}\) are the coefficients of the polynomial.</p> <p><br/></p> <p><strong>Functions</strong> <br/> Product rule: \((f(x) g(x))' = f'(x) g(x) + f(x) g'(x)\) <br/> Quotient rule: \((f(x) / g(x))' = (f'(x) g(x) - f(x) g'(x)) / (g(x))^2\) <br/> Sum rule: \((f(x) + g(x))' = f'(x) + g'(x)\) <br/> Chain rule: \((g(f(x)))' = (g o f)'(x) = g'(f(x))f'(x)\)</p> <p><br/> The generalization of the derivative to functions of several variables is the gradient. We find the gradient of the function \(f\) with respect to x by varying one variable at a time and keeping the others constant.</p> <p>Def: For a function \(f: \mathbb{R}^n \rightarrow \mathbb{R}, x \rightarrow f(x), x \in \mathbb{R}^n\) of n variables \(x_1, ..., x_n\) we define the partial derivatives as</p> <p>\(\frac{df}{dx_1}\) = \(\lim_{h \to 0} \frac{f(x_1 + h, x_2, ..., x_n) - f(x)}{h}\) <br/> … <br/> \(\frac{df}{dx_n}\) = \(\lim_{h \to 0} \frac{f(x_1, x_2, ..., x_n + h) - f(x)}{h}\)</p> <p>When we collect them in a row vector form, this is called the gradient of \(f\) or the Jacobian.</p> <p><br/> Sometimes we are interested in derivatives of higher order. Consider a function \(f: \mathbb{R}^2 \rightarrow \mathbb{R}\) of two variables x, y. We use the following notation for higher-order partial derivatives (and for gradients):</p> <p>\(\frac{d^2 f}{dx^2}\) is the second partial derivative of f with respect to x <br/> \(\frac{d^n f}{dx^n}\) is the \(n^{\text{th}}\) partial derivative of f with respect to x <br/></p> <p>The Hessian is the collection of all second-order partial derivatives. It generalizes the second derivative to multiple dimensions and is used to study the curvature of a function. It is a square matrix.</p> <p><br/></p> <p><strong>Multivariate Taylor Series</strong></p> <p>We consider a function \(f: \mathbb{R}^D \rightarrow \mathbb{R}\) \(x \rightarrow f(x), x \in \mathbb{R}^D\)</p> <p>that is smooth at \(x_0\). The multivariate Taylor series of \(f\) at \((x_0)\) is defined as</p> <p>\(f(x) = \sum_{k=0}^\infty \frac{D_x^{k}f(x_0)}{k!} (x-x_0)^k\) <br/><br/> where \({D_x^{k}f(x_0)}\) is the \(k^{\text{th}}\) total derivative of \(f\) with respect to x, evaluated at x_0. <br/><br/> The Taylor series is an infinite series that represents a function as a sum of terms based on its derivatives at a point. The Taylor series provides a precise representation of the function if it converges to the function at all points near the expansion point. <br/> The Taylor polynomial is a finite approximation of the Taylor series, using only the first few terms (up to n). A Taylor polynomial of degree n includes terms up to the \(n^{\text{th}}\) partial derivatives of the function. It provides a local approximation near a given point but becomes less accurate as you move further from that point.</p> <p><br/></p> <h4 id="probability-and-distributions">Probability and Distributions</h4> <p>The sample space Ω is the set of all possible outcomes of the experiment.</p> <p>The event space \(A\) is a subset of the sample space that represents a particular outcome or group ot outcomes we’re interested in.</p> <p>The probability \(P\). With each event, we associate a number \(P(A)\) that measures the probability that the event will occur.</p> <p><br/></p> <p>The marginal probabbility that \(X\) takes the value x irrespective of the value of random variable \(Y\) is written as p(x).</p> <p>The fraction of instances (the conditional probability) for which \(Y\) = y is written as \(p(y \mid x)\).</p> <p>Def: The <strong>product rule</strong> relates the joint distribution to the conditional distribution via p(x, y) = \(p(y \mid x)\) p(x)</p> <p><strong>Bayes’ theorem</strong> <br/> \(p(x \mid y)\) = \(\frac {p(y \mid x) p(x)}{p(y)}\)</p> <p><br/></p> <p><strong>Expectation</strong></p> <p>\(\mathbb{E}[X] = \sum_{i} x_i P(X=x_i)\) (for discrete variables)</p> <p>\(\mathbb{E}[X] = \int_{-\infty}^\infty x f_x(x) dx\) (for continuous variables)</p> <p>The expected value of a function g of a discrete random variable x is given by</p> \[\mathbb{E}[g(x)] = \sum_{x \in X} g(x) p(x)\] <p>The expected value of a function \(g: \mathbb{R} \rightarrow \mathbb{R}\) of a univariate continuous random variable x is given by</p> \[\mathbb{E}[g(x)] = \int_{x} g(x) p(x) dx\] <p><strong>Sums and Transformations of Random Variables</strong></p> \[\mathbb{E}[x+y] = \mathbb{E}[x] + \mathbb{E}[y]\] \[\mathbb{E}[x-y] = \mathbb{E}[x] - \mathbb{E}[y]\] <p><strong>Statistical Independence</strong></p> <p>Two random variables X,Y are statistically independent if and only if p(x,y) = p(x) p(y)</p> <p><strong>Gaussian Distribution</strong></p> <p>The Gaussian distribution is one of the most well-studied probability distributions for continuous-valued random variables. It’s also known as the normal distribution. For a univariate random variable, the Gaussian distribution has a density that is given by:</p> \[p(x \mid \mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x - \mu)^2}{2\sigma^2}}\]]]></content><author><name></name></author><category term="mathematics"/><summary type="html"><![CDATA[notes from Mathematics for Machine Learning by Deisenroth, Faisal, Ong]]></summary></entry><entry><title type="html">transformer architecture</title><link href="https://michaelc-yu.github.io/blog/2024/transformer/" rel="alternate" type="text/html" title="transformer architecture"/><published>2024-10-26T23:36:11+00:00</published><updated>2024-10-26T23:36:11+00:00</updated><id>https://michaelc-yu.github.io/blog/2024/transformer</id><content type="html" xml:base="https://michaelc-yu.github.io/blog/2024/transformer/"><![CDATA[<p>The transformer is made up of an encoder block and a decoder block.</p> <p>The encoder’s role is to process the input sequence in its entirety and distill it into a condensed vector representation. The encoder consits of a series of N=6 identical layers, each containing two principle sub-layers: a multi-head self-attention mechanism and a position-wise fully connected feed-forward network.</p> <p>The decoder’s role is output generation. It does this sequentially (next token prediction). The decoder also consists of N=6 identical layers. However, it includes an additional third sub-layer that facilitates multi-head attention over the encoder’s output. <br/><br/></p> <h4 id="encoder-in-depth">Encoder In-Depth</h4> <ul> <li>each word/token in the input sentence is converted into a dense vector using token embeddings</li> <li>positional encoding is added to each embedding to provide positional information. The Transformer architecture doesn’t account for sequence order, so this is necessary.</li> <li>each token embedding is linearly transformed into a query, key, and value vector through learned weight matrices. We have three parameter matrices W, K, V that we will train during training time and multiplying each of these by the input token will let us get the query, key, and value vectors.</li> <li>the model computes the dot product between each token’s query vector and each token’s key vector in the sequence to calculate attention scores. This score gives a similarity score that reflects how much attention the two tokens should give each other.</li> <li>we follow with a scaling and applying a softmax to these scores and then multiplying by the value vectors to get a weighted sum of all value vectors <br/><br/></li> </ul> <div style="text-align: center;"> <figure> <picture> <img src="/assets/img/attention.jpg" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> self-attention mechanism diagram - depicts steps 3, 4, and 5 above </div> <p><br/></p> <ul> <li>we then pass this output through a feed-forward neural network</li> <li>we stack 6 or more of these encoder layers together to form the entire encoder block <br/><br/></li> </ul> <h4 id="decoder-in-depth">Decoder In-Depth</h4> <ul> <li>convert the input tokens (which are the output tokens of the encoder so far) into token embeddings</li> <li>similar to the encoder block, we also apply positional encoding to the token embeddings</li> <li>the self-attention mechanism is applied with a masking mechanism to maintain that the decoder can only attend to earlier positions in the output sequence</li> <li>a second attention mechanism: now the Q vector comes from the decoder’s previous layer but K and V come from the encoder’s output. This attention mechanism allows the decoder to focus on relevant parts of the input sentence.</li> <li>the output of the encoder-decoder attention goes through a feed-forward network</li> <li>we stack 6 or more of these decoder layers together to form the entire decoder block <br/><br/></li> </ul> <div style="text-align: center;"> <figure> <picture> <img src="/assets/img/transformer.jpg" width="80%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> Transformer architecture from the paper "Attention Is All You Need" (Vaswani et al., 2017). </div> <p><br/><br/></p> <h4 id="positional-encodings">Positional Encodings</h4> <p><br/> Here’s a quick walkthrough on how positional encodings work and why we need them. We add positional encodings in step 2 of both the encoder and decoder step to provide positional information to the model based on each token’s relative position in the sequence. Unlike RNNs or LSTMs which process tokens sequentially, transformers process all tokens at once and so they don’t have the same built-in notion of the sense of order of token positions relative to each other.</p> <p>Positional encodings use fixed sine and cosine functions. For each token position <code class="language-plaintext highlighter-rouge">pos</code> and each dimension <code class="language-plaintext highlighter-rouge">i</code> of the embedding, the positional encoding PE(pos, i) are calculated as follows:</p> <p><strong>For even dimensions (2i):</strong> <br/><br/> \(PE(pos, 2i) = \sin\left(\frac{pos}{10000^{\frac{2i}{d}}}\right)\) <br/><br/></p> <p><strong>For odd dimensions (2i+1):</strong> <br/><br/> \(PE(pos, 2i+1) = \cos\left(\frac{pos}{10000^{\frac{2i}{d}}}\right)\)</p> <p><br/> After we calculate these positional encodings for each token embedding, we add these positional encodings directly to the embeddings for each token. This combines positional information with the original semantic meaning of each word.</p> <p>Here’s a step-by-step example. Let’s assume the input sequence is “I love transformers”</p> <div style="text-align: center;"> <figure> <picture> <img src="/assets/img/pos_enc.jpg" width="80%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>Now we just feed these three resulting vectors into the encoder or decoder block.</p>]]></content><author><name></name></author><category term="machine-learning"/><summary type="html"><![CDATA[in-depth walkthrough of the transformer architecture]]></summary></entry><entry><title type="html">neurobiology</title><link href="https://michaelc-yu.github.io/blog/2024/neuroscience/" rel="alternate" type="text/html" title="neurobiology"/><published>2024-10-26T23:36:10+00:00</published><updated>2024-10-26T23:36:10+00:00</updated><id>https://michaelc-yu.github.io/blog/2024/neuroscience</id><content type="html" xml:base="https://michaelc-yu.github.io/blog/2024/neuroscience/"><![CDATA[<hr/> <h3 id="neural-networks-in-the-brain">Neural Networks in the Brain</h3> <p><br/></p> <p>Spreading-activation model: proposes that the activation of any one concept initiates a spread of activity to nearby concepts in the network, which primes those concepts so they become temporarily more retrievable than they were before.</p> <ul> <li>ex. Sky, ocean, blueberries, sapphires all linked to “blue”</li> </ul> <p>Modern brain theories posit that memories for concepts are stored in overlapping neural circuits in the cerebral cortex. And thus priming occurs because the activation of the circuit for one concept activates the part of the circuit for another.</p> <p>Retrieval of one memory causes activation to spread out to the nearby nodes representing related memories. In neurobiology we think of neurons as being arranged in networks. The more distant the connection between neurons, the longer it should take for one neuron to activate another, and the less likely it would be for that one neuron to activate the other.</p> <figure> <picture> <img src="/assets/img/spreading.jpg" width="50%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><br/></p> <p>In other words, information is not coded in a single molecule, synapse, or neuron. Information is coded in networks and patterns of neuronal activity.</p> <p>Creativity can be thought of as networks that spread far wider, and making connections that neurons in another individual does not.</p> <hr/> <h3 id="long-term-potentiation">Long-Term Potentiation</h3> <p><br/></p> <p>Long-term potentiation (LTP) is a process involving persistent strengthening of synapses that leads to a long-lasting increase in signal transmission between neurons. It is an important process in the context of synaptic plasticity. LTP recording is widely recognized as a cellular model for the study of memory.</p> <p>LTP is like a marriage bond, connections are stronger between the presynpatic and postsynaptic neurons.</p> <p>Long Term Potentiation - the biomolecular process that your neurons go through as you learn</p> <ul> <li>how connections between neurons are strengthened through repeated pairing/firing</li> <li>neurons that fire together, wire together (using LTP)</li> </ul> <p>Our brain cells communicate with one another via synaptic transmission - one brain cell releases a chemical (neurotransmitter) that the next brain cell absorbs. This communication process is known as “neuronal firing.” When brain cells communicate frequently, the connection between them strengthens. Messages that travel the same pathway in the brain over &amp; over begin to transmit faster and faster. With enough repetition, they become automatic. That’s why we practice things like hitting a golf ball - with enough practice, we can go on automatic pilot. <br/><br/><br/> A dendritic spine is a small membranous profusion from a neuron’s dendrite that typically receives input from a single axon at the synapse.</p> <p>The more the spines are stimulated, the bigger, more stable, and longer lived they become (in contrast with smaller, immature spines, which are more transient). These large, so-called mushroom or stubby spines are thought to be one of the physical correlates of memory.</p> <figure> <picture> <img src="/assets/img/spine.jpg" width="60%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Repeated long-term potentiation causes spines to become larger and heartier. <br/><br/></p> <p>Synapses are strengthened and maintained in stages: Changes in synaptic strength that support LTP evolve in stages that can be identified by the unique molecular processes that support each stage.</p> <p>Lasting synaptic changes depend on a temporally ordered sequence of molecular events</p> <ul> <li>Generation, Stabilization, Consolidation, Maintenance</li> </ul> <p><br/></p> <p>To create permanent memories through this system, emphasize occasional activation of neural pathways. Occasional activation signals a host of maintenance molecular processes.</p>]]></content><author><name></name></author><category term="neurobiology"/><summary type="html"><![CDATA[interesting concepts from neurobiology]]></summary></entry></feed>