<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://michaelc-yu.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://michaelc-yu.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-06-18T02:24:25+00:00</updated><id>https://michaelc-yu.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Recommendation Systems Notes</title><link href="https://michaelc-yu.github.io/blog/2025/recsys/" rel="alternate" type="text/html" title="Recommendation Systems Notes"/><published>2025-05-14T13:36:11+00:00</published><updated>2025-05-14T13:36:11+00:00</updated><id>https://michaelc-yu.github.io/blog/2025/recsys</id><content type="html" xml:base="https://michaelc-yu.github.io/blog/2025/recsys/"><![CDATA[<h4 id="introduction">Introduction</h4> <p><br/></p> <p>Recommendation systems utilize machine learning models to determine user preferences based on their past history or by analyzing the preferences of similar users. These systems typically involve documents (entities a system recommends, like movies or videos), queries (information needed to make recommendations, such as user data or location), and embeddings (mappings of queries or documents to a vector space called embedding space).</p> <p>Candidate Generation: the process begins here, starting with a vast corpus and generating a smaller subset of candidates Scoring: in this step, the model ranks the candidates to select a smaller, more refined set of documents Re-ranking: this final step refines the recommendations</p> <p>Many large-scale recommendation systems implement these processes in two main stages: retrieval and ranking</p> <p><br/><br/></p> <h5 id="2-stage-recommender-systems">2-Stage Recommender Systems</h5> <p><br/> Retrieval: Quickly narrows down the vast candidate pool (millions of items) to a smaller subset using scalable models such as matrix factorization or two-tower architectures Ranking: Applies sophisticated models incorporating richer feature sets, such as user context or item-specific details, to assign relevance scores and generate a personalized, ranked list</p> <p><br/></p> <h5 id="4-stage-recommender-systems">4-Stage Recommender Systems</h5> <p><br/> Retrieval: identifies an initial set of candidates, emphasizing efficiency and broad coverage Filtering: applies business rules to exclude ineligible or irrelevant items (out-of-stock products or regionally restricted content) Scoring: uses advanced algorithms, such as deep learning models, to predict interaction likelihood by analyzing user-item interactions and contextual features Ranking (or Ordering): finalizes the recommendation list, balancing relevance, diversity, novelty, and business goals</p> <p>Eugene Yan’s 2-Stage (2x2) Model of a Recommender System Eugene Yan’s 2x2 model identifies two main dimensions within discovery systems: -Environment: offline vs. online -Process: candidate retrieval vs. ranking</p> <p>-the offline stage flows bottom-up, producing the necessary artifacts for the online environment, while the online stage processes requests left-to-right, following retrieval and ranking steps to return a final set of recommendations or search results</p> <p><br/></p> <h5 id="offline-environment">Offline Environment</h5> <p><br/> -in the offline environment, batch processes handle tasks such as model training, creating embeddings for catalog items, and developing structures like approximate nearest neighbors (ANN) indices or knowledge graphs to identify item similarity. The offline stage may also involve loading item and user data into a feature store, which helps augment input data during ranking</p> <p><br/></p> <h5 id="online-environment">Online Environment</h5> <p><br/> -the online environment serves individual user requests in real-time, utilizing artifacts generated offline (ANN indices, knowledge graphs, models). Here, input items or queries are transformed into embeddings, followed by two main steps: Candidate Retrieval: quickly narrows down millions of items to a more manageable set of candidates, trading precision for speed Ranking: ranks the reduced set of items by relevance. This stage enables the inclusion of additional features such as item and user data or contextual information, which are computationally intensive but feasible due to the smaller candidate set</p> <p><br/><br/></p> <h5 id="candidate-generation">Candidate Generation</h5> <p><br/> Candidate generation is the first stage of recommendations and is typically achieved by finding features for users that relate to features of the items.</p> <p>It’s the process of selecting a set of items that are likely to be recommended to a user. This process is typically performed after user preferences have been collected, and it involves filtering a large set of potential recommendations down to a more manageable number. The goal of candidate generation is to reduce the number of items that must be evaluated by the system, while still ensuring that the most relevant recommendations are included.</p> <p>Given a query (user info), the model generates a set of relevant candidates (videos, movies).</p> <p>There are two common candidate generation approaches: <br/> Content-based filtering: uses similarity between content to recommend new content if user watches corgi videos, the model will recommend more corgi videos <br/> Collaborative filtering: uses similarity between queries (2 or more users) and items (videos, movies) to provide recommendations if user A watches corgi videos and user A is similar to user B (in demographics and other areas), then the model can recommend corgi videos to user B even if user B has never watched a corgi video before</p> <p><br/><br/></p> <h5 id="sparse-and-dense-features">Sparse and Dense Features</h5> <p><br/> Recommender systems typically deal with two kinds of features: dense and sparse. Dense features are continuous real values, such as movie ratings or release years. Sparse features, on the other hand, are categorical and can vary in cardinality, like movie genres or the list of actors in a film. Dense features are typically the continuous variables and low dimensionality categorical variables: age, gender, number of likes in the past week, etc. Sparse features are the high dimensionality categorical variables and they are represented by an embedding: user id, page id, ad id, etc.</p> <p><br/><br/></p> <h5 id="content-based-filtering">Content-Based Filtering</h5> <p><br/> Content-Based Filtering is a sophisticated recommendation system that suggests items to users by analyzing the similarity between item features and the user’s known preferences. This approach recommends items that resemble those a user has previously liked or interacted with.</p> <p>Content-based filtering offers several advantages over alternative recommendation approaches: Effective in Data-Limited Environments: this method is particularly useful in situations where there is limited or no data on user behavior, such as in new or niche domains with a small user base Personalized Recommendations: since content-based filtering relies on an individual user’s preferences rather than the collective behavior of other users, it can provide highly personalized recommendations Scalability: the model does not require data from other users, focusing solely on the current user’s information, which makes it easier to scale Recommendation of Niche Items: it can recommend niche items tailored to each user’s unique preferences, items that might not be of interest to a broader audience Ability to Recommend New Items: the method can recommend newly introduced items without waiting for user interaction data, as the recommendations are based on the item’s inherent features Capturing Unique User Interests: content-based filtering excels at capturing and catering to the distinct interests of users by recommending items based on their previous engagements</p> <p>Limitations of Content-Based Filtering <br/> Need for domain knowledge: features must often be manually engineered. Consequently, the effectiveness of the model is closely tied to the quality of these hand-engineered features Limited exploration of new interests: the model tends to recommend items within the user’s existing preferences, limiting its ability to introduce new or diverse interests Difficulty in discovering new interests <br/> Dependence on feature availability: method may be ineffective in situations where there is a lack of detailed item information or where items possess limited attributes or features</p> <p><br/><br/></p> <h5 id="collaborative-filtering">Collaborative Filtering</h5> <p><br/> A recommendation system technique used to suggest items to users based on the behaviors and preferences of a broader user group. It operates on the principle that users with similar tastes are likely to enjoy similar items.</p> <p>Unlike content-based filtering, collaborative filtering automatically learns embeddings without relying on manually engineered features.</p> <p>Advantages of collaborative filtering <br/> -Effectiveness in sparse data environments: collaborative filtering is particularly effective when there is limited information about item attributes, as it relies on user behavior rather than item features <br/> -Scalability: it performs well in environments with a large and diverse user base, making it suitable for systems with extensive datasets <br/> -Serendipitous Recommendations: the system can provide unexpected yet relevant recommendations, helping users discover items they might not have found independently <br/> -No Domain Knowledge Required: it does not require specific knowledge of item features, eliminating the need for domain-specific expertise in engineering these features <br/> -Efficiency: collaborative filtering models are generally faster and less computationally intensive than content-based filtering, as they do not require analysis of item-specific attributes</p> <p><br/></p> <p>Disadvantages of collaborative filtering <br/> -Data scarcity: it can struggle in situations where there is insufficient data on user behavior, limiting its effectiveness <br/> -Unique preferences: the system may have difficulty making accurate recommendations for users with highly unique or niche preferences, as it relies on similarities between users <br/> -Cold start problem: new items or users with limited data pose challenges for generating accurate recommendations due to the lack of historical interaction data <br/> -Difficulty handling niche interests</p> <p><br/><br/></p> <h5 id="matrix-factorization-mf">Matrix Factorization (MF)</h5> <p><br/> -Matrix Factorization (MF) is a simple embedding model. The algorithm performs a decomposition of the (sparse) user-item feedback matrix into the product of two (dense) lower-dimensional matrices. One matrix represents the user embeddings, while the other represents the item embeddings. In essence, the model learns to map each user to an embedding vector and similarly maps each item to an embedding vector, such that the distance between these vectors reflects their relevance.</p> <p>-as part of training, we aim to produce user and item embedding matrices so that their product is a good approximation of the feedback matrix. Aka when you multiply the two matrices you get a good approximation of the sparse matrix back.</p> <p>It takes a large, sparse user-item interaction matrix (R) and factorizes it into two lower-dimensional matrices.</p> <p>You multiply the full matrices to get the entire predicted matrix, but each individual prediction is a dot product between a user embedding and an item embedding. The dot product between the u-th user vector and the i-th item vector. It involves matrix multiplication globally, but it’s made up of dot products between user and item embeddings at the individual prediction level.</p> <p>Why do this? <br/> Fill in missing values: <br/> -the original matrix is super sparse (most users haven’t rated most items) <br/> -after factorization, the dot product between user and item embeddings gives a predicted rating or interaction score <br/> Learn latent structure: <br/> -learns hidden patterns behind why users like certain items <br/> Enable recommendations <br/> -once you have embeddings for all users and items, you can recommend top-k items, find similar users/items via vector similarity</p> <p><br/><br/></p> <h5 id="neural-collaborative-filtering">Neural Collaborative Filtering</h5> <p><br/> A type of recommendation system that uses neural networks to learn user-item interactions and predict preferences. It generalizes matrix factorization, a common technique in collaborative filtering, by replacing the dot product with a neural network architecture. This allows it to capture more complex and non-linear relationships between users and items. It takes a user ID and item ID and their embeddings. It concatenates the user and item embeddings and then feeds that through a MLP. The output is the predicted interaction score (e.g., click probability).</p> <p><br/><br/></p> <h5 id="implicit-vs-explicit-feedback-in-recsys">Implicit vs. Explicit Feedback in RecSys</h5> <p><br/> Explicit feedback is users directly give ratings (e.g. 4 stars, thumbs up). Data is clear, but sparser. <br/> Implicit feedback is inferred from behavior (e.g. clicks, watch time, purchases, skips). Much more abundant, but noisy—just because someone watched something doesn’t mean they liked it.</p> <p><br/><br/></p> <h5 id="deep-neural-network-based-recommendations">Deep Neural Network Based Recommendations</h5> <p><br/> -more ability to learn complex patterns in user behavior and item attributes. <br/> -One common approach for DNN-based recommendation is to use a matrix factorization model as a baseline and then incorporate additional layers of neural networks to capture more complex patterns in the user-item interactions <br/> -Another popular approach for DNN-based recommendation is to use a sequence modeling architecture, such as a RNN or a transformer network. These models can capture temporal dependencies in user behavior and item popularity, allowing for more accurate and personalized recommendations. For example, an RNN can be used to model the sequence of items that a user has interacted with over time, and then use this information to predict which item the user is likely to interact with next</p> <p><br/><br/></p> <h5 id="two-tower-model">Two-tower Model</h5> <p><br/> -many online platforms, like yt, facebook, and tiktok, use the two-tower model in their recommender system <br/> -the two-tower model consists of two sub-neural networks: query and item <br/> -the query tower encodes user data; the item tower encodes product data <br/> -the output of each tower is an embedding, i.e., a dense vector <br/> -the similarity of a user and product pair is measured using the dot product <br/> -the trained embeddings of query and item towers are stored for fast retrieval <br/> In a two-tower architecture, there are two towers or columns: one tower represents the user and the other tower represents the item. Each tower consists of multiple layers of neurons, which can be fully connected or sparse. The user tower takes as input the user’s features, such as demographic information, browsing history, or social network connections, and processes them through the layers to produce a user embedding, which is a low-dimensional vector representation of the user. The item tower takes as input the item’s features, such as its genre, cast, or director, and processes them through the layers to produce an item embedding, which is a low-dimensional vector representation of the item. The user and item embeddings are then combined using a similarity function, such as dot product or cosine similarity, to produce a score that represents the predicted rating or likelihood of interaction between the user and the item.</p> <p><br/><br/></p> <h5 id="candidate-retrieval">Candidate Retrieval</h5> <p><br/> -Now that you have an embedding model, how would you decide which items to recommend given a user? <br/> -at serve time, given a query, you start by doing one of the following: <br/> -for a matrix factorization model, the query (or user) embedding is known statically, and the system can simply look it up from the user embedding matrix <br/> -for a DNN model, the system computes the query embedding at serve time by running the network on the feature vector x <br/> -once you have the query embedding q, search for item embeddings that are close to q in the embedding space. This is a nearest neighbor problem. <br/></p> <p>Large-scale Retrieval <br/> -to compute the nearest neighbors in the embedding space, the system can exhaustively score every potential candidate. Exhausting scoring can be expensive for very large corpora, but you can use either of the following strategies to make it more efficient: -if the query embedding is known statically, the system can perform exhaustive scoring offline, pre-computing and storing a list of the top candidates for each query. This is a common practice for related-item recommendation -use approximate nearest neighbors</p> <p><br/><br/></p> <h4 id="metrics">Metrics</h4> <p><br/></p> <h5 id="click-through-rate-ctr">Click-Through Rate (CTR)</h5> <p><br/> Out of all the times we showed a particular item (or group of items), how often did users click on it?</p> <p><br/></p> <h5 id="ndcg-normalized-discounted-cumulative-gain">NDCG (Normalized Discounted Cumulative Gain)</h5> <p><br/> NDCG measures how well your ranking model orders items, taking into account both relevance and position in the list. The idea: Relevant items at the top of the list should get more credit than relevant items buried at the bottom. NDCG Evaluates your model’s ordering, not just which items are present</p> <p><br/></p> <h5 id="recallk">Recall@K</h5> <p><br/> Out of all the relevant items for this user, how many did my system successfully surface in the top K ranked list?</p> <p>In practice relevance is defined from logs</p> <p>Let’s say you want to compute recall@10 offline. You could define “relevant” items as: <br/> -items the user clicked in the past 24 hours <br/> -items they watched for &gt;30s in the past week <br/> -items they gave high ratings to in past data <br/> Then you ask: “Did our top-10 recommendation list contain any of these?”</p> <p><br/></p> <h5 id="map-mean-average-precision">MAP (Mean Average Precision)</h5> <p><br/> Measures how well the recommendation system ranks relevant items near the top of the recommendation list across all users. It’s useful when you want to reward systems that rank relevant items higher. You care not just what items are recommended, but where in the list they appear.</p> <p><br/></p> <h5 id="ndcg-vs-map-intuition">NDCG vs. MAP intuition:</h5> <p><br/> -NDCG is how well did you rank the most important items near the top? <br/> -MAP is how many of your top-ranked items were correct, and how early did you get them?</p> <p><br/><br/></p> <h5 id="other-stuff">Other stuff</h5> <p><br/> Diversity is usually injected during re-ranking, not the initial ranking stage</p> <p>Genre constraint: “no more than 2 items of the same genre in the top 5”</p> <p>Dot product is efficient and works well when user and item embeddings live in the same space and relevance is a function of similarity. But in cases where preferences are more conditional or nonlinear, like job or dating recommendations, we’d use deep interaction models instead to model richer user-item relationships.</p> <p>Popularity Bias: popular items get recommended more often. Less popular items are overlooked even if some users would love them. System tends to reinforce already popular content (rich get richer).</p> <p>Exposure Bias: you can’t recommend what users have never been shown. Just because a user didn’t interact with something doesn’t mean they disliked it — maybe they just never saw it.</p> <p>Both popularity and exposure bias lead to feedback loops and reduced discovery.</p> <p>Bandits are designed to maximize reward over time — but if not properly regularized, they can actually become greedy over a few high-reward arms, which reduces diversity. If the bandit finds that a few items consistently perform well, it might just keep recommending those over and over. Result is that popular items get even more exposure, while other potentially good items never get a chance.</p> <p><br/><br/></p> <h5 id="ranking--scoring">Ranking / Scoring</h5> <p><br/> Scoring refers to the process of assigning a score or a rating to each item in the candidate pool based on its similarity to the user’s preferences or past behavior. The scoring function is used to determine the relevance of each item to the user, and items with higher scores are considered to be more relevant.</p> <p>Ranking, on the other hand, is the process of ordering the items based on their scores. The items with the highest scores are ranked at the top of the recommendation list, while the items with the lowest scores are ranked at the bottom. The ranking process ensures that the most relevant items are presented to the user first.</p> <p>Let’s look at different methods and techniques used for scoring.</p> <p><br/><br/></p> <h5 id="re-ranking">Re-ranking</h5> <p><br/> In the final phase of a recommendation system, the system has the capability to re-rank candidates by taking into account additional criteria or constraints that were not initially considered. This re-ranking stage allows the system to assess factors beyond the initial scoring, such as the diversity of items within the recommended set.</p> <p><br/><br/></p> <h5 id="popular-architectures">Popular Architectures</h5> <p><br/> The wide and deep architecture is a model architecture that combines a linear model (wide) and a deep neural network (deep), and trains them together to make predictions.</p> <p>The “Wide” Part = memorization -think of it like manual features + sparse cross-products -implemented via logistic regression or linear models with sparse features</p> <p>The “Deep” Part = generalization -a deep neural network that takes in embeddings of user, item, and context -learns nonlinear patterns and abstract interactions -helps generalize to unseen combinations of features</p> <p>The wide model helps memorize known patterns (co-viewed items, past behaviors) The deep model helps predict and generalize to new behaviors During training, both are optimized together — the outputs are combined (via sum or weighted average)</p> <p>Latent factor models embed users and items into a shared vector space, where their similarity reflects the likelihood of interaction. They’re commonly trained using matrix factorization and are powerful for collaborative filtering, especially when explicit user-item interaction data is available</p> <p>Knowledge graphs in recommender systems represent entities like users, items, genres, actors, or creators as nodes, and their relationships (e.g., “acted in,” “belongs to genre,” “watched by”) as edges in a graph structure. This enables the model to capture rich, structured, and explainable connections beyond simple collaborative filtering. By traversing multi-hop paths (e.g., user → watched movie A → has actor X → acted in movie B), the system can recommend semantically relevant items, even in cold-start scenarios where historical interaction data is sparse. Knowledge graphs improve personalization, support content diversity, and enhance explainability, and are often used alongside traditional models or embedded into neural architectures like graph neural networks (GNNs) or meta-path based embeddings to power more intelligent and context-aware recommendations.</p> <p>Pointwise ranking predicts absolute engagement scores for each item independently, using losses like cross-entropy or regression. Pairwise ranking, on the other hand, models the relative preference between item pairs — optimizing for which item should rank higher — and is often trained with BPR loss. Pairwise is more directly aligned with ranking quality, but pointwise is simpler and commonly used in production.</p> <p><br/><br/></p> <h5 id="multi-armed-bandits">Multi-armed Bandits</h5> <p><br/> Imagine you have several slot machines (or “arms”) and each one gives a different reward when pulled. The goal is to maximize the total reward by pulling the most rewarding arms. However, you don’t know beforehand which arm is the best. Exploration vs. Exploitation: you need to balance Exploration: trying out different arms to learn which one gives the best reward Exploitation: pulling the arm that seems to give the best reward based on what you’ve learned so far</p> <p>Bandits offer advantages over batch machine learning and A/B testing methods, as they minimize regret by continuously learning and adapting recommendations without the need for extensive data collection or waiting for test results. They can provide better performance in situations with limited data or when dealing with long-tail or cold-start scenarios, where batch recommenders may favor popular items over potentially relevant but lesser-known options.</p> <p><br/><br/></p> <h5 id="contextual-bandits">Contextual Bandits</h5> <p><br/> Contextual bandits extend the multi-armed bandit problem by introducing context into the decision process. Rather than just choosing an arm based on past reward alone, contextual bandits take into account the current situation or “context” before making a decision. Context: the “context” refers to additional information that can help in decision-making. For example, in a news website, the context might include user information such as location, browsing history, or device type. Action: based on the context, the algorithm decides which action to take (which arm to pull, or which recommendation to make) Reward: after the action, the algorithm observes the reward (whether the user clicked the recommended item or not) and uses this feedback to improve future decisions</p> <p>Contextual Bandits vs. Full Reinforcement Learning <br/> -contextual bandits are a simplification of full reinforcement learning. In full RL, the agent needs to learn from sequences of actions and long-term rewards (i.e., the environment’s state changes over time). In contrast, contextual bandits only deal with one-time, immediate rewards and do not require learning long-term strategies. This makes them computationally simpler and ideal for situations where immediate feedback is available, like in recommender systems.</p> <p>Online learning <br/> Contextual bandits are highly suited for online learning, meaning they can adapt to user behavior in real-time. The system can continuously refine its recommendations as it gathers more feedback from the user, making it particularly effective in dynamic environments.</p> <p><br/><br/></p> <h4 id="system-design-for-recommendations-and-search">System Design for Recommendations and Search</h4> <p><br/></p> <p>The offline environment largely hosts batch processes such as model training (e.g., representation learning, ranking), creating embeddings for catalog items, and building an approximate nearest neighbors (ANN) index or knowledge graph to find similar items. It may also include loading item and user data into a feature store that is used to augment input data during ranking.</p> <p>The online environment then uses the artifacts generated (e.g., ANN indices, knowledge graphs, models, feature stores) to serve individual requests. A typical approach is converting the input item or search query into an embedding, followed by candidate retrieval and ranking. There are also other preprocessing steps (e.g., standardizing queries, tokenization, spell check) and post-processing steps (e.g., filtering undesirable items, business logic) though we won’t discuss them in this writeup.</p> <p>Candidate retrieval is a fast—but coarse—step to narrow down millions of items into hundreds of candidates. We trade off precision for efficiency to quickly narrow the search space (e.g., from millions to hundreds, a 99.99% reduction) for the downstream ranking task. Most contemporary retrieval methods convert the input (i.e., item, search query) into an embedding before using ANN to find similar items. Nonetheless, in the examples below, we’ll also see systems using graphs (DoorDash) and decision trees (LinkedIn).</p> <p>Ranking is a slower—but more precise—step to score and rank top candidates. As we’re processing fewer items (i.e., hundreds instead of millions), we have room to add features that would have been infeasible in the retrieval step (due to compute and latency constraints). Such features include item and user data, and contextual information. We can also use more sophisticated models with more layers and parameters.</p> <p>In the offline environment, data flows bottom-up, where we use training data and item/user data to create artifacts such as models, ANN indices, and feature stores. These artifacts are then loaded into the online environment (via the dashed arrows). In the online environment, each request flows left to right, through the retrieval and ranking steps before returning a set of results (e.g., recommendations, search results).</p>]]></content><author><name></name></author><category term="machine-learning"/><summary type="html"><![CDATA[notes from studying recommender systems]]></summary></entry><entry><title type="html">Math Equations &amp;amp; Insights</title><link href="https://michaelc-yu.github.io/blog/2024/math/" rel="alternate" type="text/html" title="Math Equations &amp;amp; Insights"/><published>2024-11-24T13:36:11+00:00</published><updated>2024-11-24T13:36:11+00:00</updated><id>https://michaelc-yu.github.io/blog/2024/math</id><content type="html" xml:base="https://michaelc-yu.github.io/blog/2024/math/"><![CDATA[<h4 id="vector-calculus">Vector Calculus</h4> <p>Taylor Polynomial is an approximation of a function f(x) around a point \(x_0\) using a polynomial constructed from the derivatives of f(x) at that point.</p> <p>Def: The Taylor Polynomial of degree n of \(f: \mathbb{R} \rightarrow \mathbb{R}\) at \(x_0\) is defined as <br/> \(T_n(x) = \sum_{k=0}^n \frac{f^{(k)}(x_0)}{k!} (x-x_0)^k\) <br/><br/> where \(f^{(k)}(x_0)\) is the \(k^{\text{th}}\) derivative of f at \(x_0\) and \(\frac{f^{(k)}(x_0)}{k!}\) are the coefficients of the polynomial.</p> <p><br/></p> <p><strong>Functions</strong> <br/> Product rule: \((f(x) g(x))' = f'(x) g(x) + f(x) g'(x)\) <br/> Quotient rule: \((f(x) / g(x))' = (f'(x) g(x) - f(x) g'(x)) / (g(x))^2\) <br/> Sum rule: \((f(x) + g(x))' = f'(x) + g'(x)\) <br/> Chain rule: \((g(f(x)))' = (g o f)'(x) = g'(f(x))f'(x)\)</p> <p><br/> The generalization of the derivative to functions of several variables is the gradient. We find the gradient of the function \(f\) with respect to x by varying one variable at a time and keeping the others constant.</p> <p>Def: For a function \(f: \mathbb{R}^n \rightarrow \mathbb{R}, x \rightarrow f(x), x \in \mathbb{R}^n\) of n variables \(x_1, ..., x_n\) we define the partial derivatives as</p> <p>\(\frac{df}{dx_1}\) = \(\lim_{h \to 0} \frac{f(x_1 + h, x_2, ..., x_n) - f(x)}{h}\) <br/> … <br/> \(\frac{df}{dx_n}\) = \(\lim_{h \to 0} \frac{f(x_1, x_2, ..., x_n + h) - f(x)}{h}\)</p> <p>When we collect them in a row vector form, this is called the gradient of \(f\) or the <strong>Jacobian</strong>.</p> <p><br/> Sometimes we are interested in derivatives of higher order. Consider a function \(f: \mathbb{R}^2 \rightarrow \mathbb{R}\) of two variables x, y. We use the following notation for higher-order partial derivatives (and for gradients):</p> <p>\(\frac{d^2 f}{dx^2}\) is the second partial derivative of f with respect to x <br/> \(\frac{d^n f}{dx^n}\) is the \(n^{\text{th}}\) partial derivative of f with respect to x <br/></p> <p>The Hessian is the collection of all second-order partial derivatives. It generalizes the second derivative to multiple dimensions and is used to study the curvature of a function. It is a square matrix.</p> <p><br/></p> <p><strong>Multivariate Taylor Series</strong></p> <p>We consider a function \(f: \mathbb{R}^D \rightarrow \mathbb{R}\) \(x \rightarrow f(x), x \in \mathbb{R}^D\)</p> <p>that is smooth at \(x_0\). The multivariate Taylor series of \(f\) at \((x_0)\) is defined as</p> <p>\(f(x) = \sum_{k=0}^\infty \frac{D_x^{k}f(x_0)}{k!} (x-x_0)^k\) <br/><br/> where \({D_x^{k}f(x_0)}\) is the \(k^{\text{th}}\) total derivative of \(f\) with respect to x, evaluated at \(x_0\). <br/><br/> The Taylor series is an infinite series that represents a function as a sum of terms based on its derivatives at a point. The Taylor series provides a precise representation of the function if it converges to the function at all points near the expansion point. <br/> The Taylor polynomial is a finite approximation of the Taylor series, using only the first few terms (up to n). A Taylor polynomial of degree n includes terms up to the \(n^{\text{th}}\) partial derivatives of the function. It provides a local approximation near a given point but becomes less accurate as you move further from that point.</p> <p><br/></p> <h4 id="probability-and-distributions">Probability and Distributions</h4> <p>The sample space Ω is the set of all possible outcomes of the experiment.</p> <p>The event space \(A\) is a subset of the sample space that represents a particular outcome or group ot outcomes we’re interested in.</p> <p>The probability \(P\). With each event, we associate a number \(P(A)\) that measures the probability that the event will occur.</p> <p><br/></p> <p>The marginal probabbility that \(X\) takes the value x irrespective of the value of random variable \(Y\) is written as p(x).</p> <p>The fraction of instances (the conditional probability) for which \(Y\) = y is written as \(p(y \mid x)\).</p> <p>Def: The <strong>product rule</strong> relates the joint distribution to the conditional distribution via p(x, y) = \(p(y \mid x)\) p(x)</p> <p><strong>Bayes’ theorem</strong> <br/> \(p(x \mid y)\) = \(\frac {p(y \mid x) p(x)}{p(y)}\)</p> <p><br/></p> <p><strong>Expectation</strong></p> <p>\(\mathbb{E}[X] = \sum_{i} x_i P(X=x_i)\) (for discrete variables)</p> <p>\(\mathbb{E}[X] = \int_{-\infty}^\infty x f_x(x) dx\) (for continuous variables)</p> <p>The expected value of a function g of a discrete random variable x is given by</p> \[\mathbb{E}[g(x)] = \sum_{x \in X} g(x) p(x)\] <p>The expected value of a function \(g: \mathbb{R} \rightarrow \mathbb{R}\) of a univariate continuous random variable x is given by</p> \[\mathbb{E}[g(x)] = \int_{x} g(x) p(x) dx\] <p><strong>Sums and Transformations of Random Variables</strong></p> \[\mathbb{E}[x+y] = \mathbb{E}[x] + \mathbb{E}[y]\] \[\mathbb{E}[x-y] = \mathbb{E}[x] - \mathbb{E}[y]\] <p><strong>Covariance</strong></p> <p>The covariance intuitively represents the notion of how dependent random variables are to one another.</p> <p>The covariance bewteen two univariate random variables X, Y \(\in \mathbb{R}\) is given by the expected product of their derivatives from their respective means,</p> \[Cov_{\text{x,y}} [x, y] = \mathbb{E}_{\text{x,y}} [(x-\mathbb{E}_x[x]) (y-\mathbb{E}_y[y])]\] <p>The covariance of a variable with itself is called the variance and is denoted by \(V_x[x]\). The square root of the variance is called the standard deviation \(\sigma(x)\)</p> <p>If we consider two multivariate random variables X and Y with states \(x \in \mathbb{R}^D\) and \(y \in \mathbb{R}^E\) respectively, the covariance between X and Y is defined as</p> <p>Cov[x, y] = \(\mathbb{E}[xy^T] - \mathbb{E}[x]\mathbb{E}[y]^T = Cov[y, x]^T \in \mathbb{R}^{\text{D x E}}\)</p> <p><strong>Correlation</strong></p> <p>The correlation between two random variables X, Y is given by</p> \[Corr[x, y] = \frac{Cov[x,y]}{\sqrt{V[x] V[y]}} \in [-1, 1]\] <p>The covariance (and correlation) indicate how two random variables are related. Positive correlation means that when x grows, then y is also expected to grow. Negative correlation means that as x increases, then y decreases.</p> <p><strong>Statistical Independence</strong></p> <p>Two random variables X,Y are statistically independent if and only if p(x,y) = p(x) p(y)</p> <p><strong>Gaussian Distribution</strong></p> <p>The Gaussian distribution is one of the most well-studied probability distributions for continuous-valued random variables. It’s also known as the normal distribution. For a univariate random variable, the Gaussian distribution has a density that is given by:</p> \[p(x \mid \mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x - \mu)^2}{2\sigma^2}}\] <h4 id="continuous-optimization">Continuous Optimization</h4> <p><strong>Gradient Descent</strong></p> <p>If we want to find a local optimum \(f(x_0)\) of a function \(f: \mathbb{R}^n \rightarrow \mathbb{R}, x \rightarrow f(x)\), we start with an initial guess \(x_0\) of the parameters we wish to optimize and then iterate according to</p> \[x_{\text{i+1}} = x_i - y_i ((\nabla{f}) (x_i))^T\] <p>For a suitable step-size \(y_i\), the sequence \(f(x_0) \geq f(x_1) \geq ...\) converges to a local minimum.</p> <p><strong>Convex Optimization</strong></p> <p>A <strong>convex function</strong> is a type of function with a specific shape and property that is crucial in optimization. Intuitively, a convex function has a bowl-shaped or U-shaped curve, which makes it easier to find the minimum of the function because any local minimum is also a global minimum.</p> <p>A set C is a <strong>convex set</strong> if for any \(x, y \in C\) and for any scalar \(\theta\) with \(0 \leq \theta \leq 1\), we have</p> \[\theta x + (1 - \theta) y \in C\] <p>Convex sets are sets such that a straight line connecting any two elements of the set lie inside the set.</p> <p><strong>Convex functions</strong> are functions such that a straight line between any two points of the function lie above the function.</p>]]></content><author><name></name></author><category term="mathematics"/><summary type="html"><![CDATA[notes from Mathematics for Machine Learning by Deisenroth, Faisal, Ong]]></summary></entry><entry><title type="html">Transformer Architecture</title><link href="https://michaelc-yu.github.io/blog/2024/transformer/" rel="alternate" type="text/html" title="Transformer Architecture"/><published>2024-10-26T23:36:11+00:00</published><updated>2024-10-26T23:36:11+00:00</updated><id>https://michaelc-yu.github.io/blog/2024/transformer</id><content type="html" xml:base="https://michaelc-yu.github.io/blog/2024/transformer/"><![CDATA[<p>The transformer is made up of an encoder block and a decoder block.</p> <p>The encoder’s role is to process the input sequence in its entirety and distill it into a condensed vector representation. The encoder consits of a series of N=6 identical layers, each containing two principle sub-layers: a multi-head self-attention mechanism and a position-wise fully connected feed-forward network.</p> <p>The decoder’s role is output generation. It does this sequentially (next token prediction). The decoder also consists of N=6 identical layers. However, it includes an additional third sub-layer that facilitates multi-head attention over the encoder’s output. <br/><br/></p> <h4 id="encoder-in-depth">Encoder In-Depth</h4> <ul> <li>each word/token in the input sentence is converted into a dense vector using token embeddings</li> <li>positional encoding is added to each embedding to provide positional information. The Transformer architecture doesn’t account for sequence order, so this is necessary.</li> <li>each token embedding is linearly transformed into a query, key, and value vector through learned weight matrices. We have three parameter matrices W, K, V that we will train during training time and multiplying each of these by the input token will let us get the query, key, and value vectors.</li> <li>the model computes the dot product between each token’s query vector and each token’s key vector in the sequence to calculate attention scores. This score gives a similarity score that reflects how much attention the two tokens should give each other.</li> <li>we follow with a scaling and applying a softmax to these scores and then multiplying by the value vectors to get a weighted sum of all value vectors <br/><br/></li> </ul> <div style="text-align: center;"> <figure> <picture> <img src="/assets/img/attention.jpg" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> self-attention mechanism diagram - depicts steps 3, 4, and 5 above </div> <p><br/></p> <ul> <li>we then pass this output through a feed-forward neural network</li> <li>we stack 6 or more of these encoder layers together to form the entire encoder block <br/><br/></li> </ul> <h4 id="decoder-in-depth">Decoder In-Depth</h4> <ul> <li>convert the input tokens (which are the output tokens of the encoder so far) into token embeddings</li> <li>similar to the encoder block, we also apply positional encoding to the token embeddings</li> <li>the self-attention mechanism is applied with a masking mechanism to maintain that the decoder can only attend to earlier positions in the output sequence</li> <li>a second attention mechanism: now the Q vector comes from the decoder’s previous layer but K and V come from the encoder’s output. This attention mechanism allows the decoder to focus on relevant parts of the input sentence.</li> <li>the output of the encoder-decoder attention goes through a feed-forward network</li> <li>we stack 6 or more of these decoder layers together to form the entire decoder block <br/><br/></li> </ul> <div style="text-align: center;"> <figure> <picture> <img src="/assets/img/transformer.jpg" width="80%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> Transformer architecture from the paper "Attention Is All You Need" (Vaswani et al., 2017). </div> <p><br/><br/></p> <h4 id="positional-encodings">Positional Encodings</h4> <p><br/> Here’s a quick walkthrough on how positional encodings work and why we need them. We add positional encodings in step 2 of both the encoder and decoder step to provide positional information to the model based on each token’s relative position in the sequence. Unlike RNNs or LSTMs which process tokens sequentially, transformers process all tokens at once and so they don’t have the same built-in notion of the sense of order of token positions relative to each other.</p> <p>Positional encodings use fixed sine and cosine functions. For each token position <code class="language-plaintext highlighter-rouge">pos</code> and each dimension <code class="language-plaintext highlighter-rouge">i</code> of the embedding, the positional encoding PE(pos, i) are calculated as follows:</p> <p><strong>For even dimensions (2i):</strong> <br/><br/> \(PE(pos, 2i) = \sin\left(\frac{pos}{10000^{\frac{2i}{d}}}\right)\) <br/><br/></p> <p><strong>For odd dimensions (2i+1):</strong> <br/><br/> \(PE(pos, 2i+1) = \cos\left(\frac{pos}{10000^{\frac{2i}{d}}}\right)\)</p> <p><br/> After we calculate these positional encodings for each token embedding, we add these positional encodings directly to the embeddings for each token. This combines positional information with the original semantic meaning of each word.</p> <p>Here’s a step-by-step example. Let’s assume the input sequence is “I love transformers”</p> <div style="text-align: center;"> <figure> <picture> <img src="/assets/img/pos_enc.jpg" width="80%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>Now we just feed these three resulting vectors into the encoder or decoder block.</p>]]></content><author><name></name></author><category term="machine-learning"/><summary type="html"><![CDATA[in-depth walkthrough of the transformer architecture]]></summary></entry><entry><title type="html">Neurobiology</title><link href="https://michaelc-yu.github.io/blog/2024/neuroscience/" rel="alternate" type="text/html" title="Neurobiology"/><published>2024-10-26T23:36:10+00:00</published><updated>2024-10-26T23:36:10+00:00</updated><id>https://michaelc-yu.github.io/blog/2024/neuroscience</id><content type="html" xml:base="https://michaelc-yu.github.io/blog/2024/neuroscience/"><![CDATA[<hr/> <h3 id="neural-networks-in-the-brain">Neural Networks in the Brain</h3> <p><br/></p> <p>Spreading-activation model: proposes that the activation of any one concept initiates a spread of activity to nearby concepts in the network, which primes those concepts so they become temporarily more retrievable than they were before.</p> <ul> <li>ex. Sky, ocean, blueberries, sapphires all linked to “blue”</li> </ul> <p>Modern brain theories posit that memories for concepts are stored in overlapping neural circuits in the cerebral cortex. And thus priming occurs because the activation of the circuit for one concept activates the part of the circuit for another.</p> <p>Retrieval of one memory causes activation to spread out to the nearby nodes representing related memories. In neurobiology we think of neurons as being arranged in networks. The more distant the connection between neurons, the longer it should take for one neuron to activate another, and the less likely it would be for that one neuron to activate the other.</p> <figure> <picture> <img src="/assets/img/spreading.jpg" width="50%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><br/></p> <p>In other words, information is not coded in a single molecule, synapse, or neuron. Information is coded in networks and patterns of neuronal activity.</p> <p>Creativity can be thought of as networks that spread far wider, and making connections that neurons in another individual does not.</p> <hr/> <h3 id="long-term-potentiation">Long-Term Potentiation</h3> <p><br/></p> <p>Long-term potentiation (LTP) is a process involving persistent strengthening of synapses that leads to a long-lasting increase in signal transmission between neurons. It is an important process in the context of synaptic plasticity. LTP recording is widely recognized as a cellular model for the study of memory.</p> <p>LTP is like a marriage bond, connections are stronger between the presynpatic and postsynaptic neurons.</p> <p>Long Term Potentiation - the biomolecular process that your neurons go through as you learn</p> <ul> <li>how connections between neurons are strengthened through repeated pairing/firing</li> <li>neurons that fire together, wire together (using LTP)</li> </ul> <p>Our brain cells communicate with one another via synaptic transmission - one brain cell releases a chemical (neurotransmitter) that the next brain cell absorbs. This communication process is known as “neuronal firing.” When brain cells communicate frequently, the connection between them strengthens. Messages that travel the same pathway in the brain over &amp; over begin to transmit faster and faster. With enough repetition, they become automatic. That’s why we practice things like hitting a golf ball - with enough practice, we can go on automatic pilot. <br/><br/><br/> A dendritic spine is a small membranous profusion from a neuron’s dendrite that typically receives input from a single axon at the synapse.</p> <p>The more the spines are stimulated, the bigger, more stable, and longer lived they become (in contrast with smaller, immature spines, which are more transient). These large, so-called mushroom or stubby spines are thought to be one of the physical correlates of memory.</p> <figure> <picture> <img src="/assets/img/spine.jpg" width="60%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Repeated long-term potentiation causes spines to become larger and heartier. <br/><br/></p> <p>Synapses are strengthened and maintained in stages: Changes in synaptic strength that support LTP evolve in stages that can be identified by the unique molecular processes that support each stage.</p> <p>Lasting synaptic changes depend on a temporally ordered sequence of molecular events</p> <ul> <li>Generation, Stabilization, Consolidation, Maintenance</li> </ul> <p><br/></p> <p>To create permanent memories through this system, emphasize occasional activation of neural pathways. Occasional activation signals a host of maintenance molecular processes.</p>]]></content><author><name></name></author><category term="neurobiology"/><summary type="html"><![CDATA[interesting concepts from neurobiology]]></summary></entry></feed>