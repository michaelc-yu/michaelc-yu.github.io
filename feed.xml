<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://michaelc-yu.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://michaelc-yu.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-11-05T06:25:03+00:00</updated><id>https://michaelc-yu.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">neurobiology</title><link href="https://michaelc-yu.github.io/blog/2024/neuroscience/" rel="alternate" type="text/html" title="neurobiology"/><published>2024-10-26T23:36:10+00:00</published><updated>2024-10-26T23:36:10+00:00</updated><id>https://michaelc-yu.github.io/blog/2024/neuroscience</id><content type="html" xml:base="https://michaelc-yu.github.io/blog/2024/neuroscience/"><![CDATA[<hr/> <h3 id="neural-networks-in-the-brain">Neural Networks in the Brain</h3> <p><br/></p> <p>Spreading-activation model: proposes that the activation of any one concept initiates a spread of activity to nearby concepts in the network, which primes those concepts so they become temporarily more retrievable than they were before.</p> <ul> <li>ex. Sky, ocean, blueberries, sapphires all linked to “blue”</li> </ul> <p>Modern brain theories posit that memories for concepts are stored in overlapping neural circuits in the cerebral cortex. And thus priming occurs because the activation of the circuit for one concept activates the part of the circuit for another.</p> <p>Retrieval of one memory causes activation to spread out to the nearby nodes representing related memories. In neurobiology we think of neurons as being arranged in networks. The more distant the connection between neurons, the longer it should take for one neuron to activate another, and the less likely it would be for that one neuron to activate the other.</p> <figure> <picture> <img src="/assets/img/spreading.jpg" width="50%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><br/></p> <p>In other words, information is not coded in a single molecule, synapse, or neuron. Information is coded in networks and patterns of neuronal activity.</p> <p>Creativity can be thought of as networks that spread far wider, and making connections that neurons in another individual does not.</p> <hr/> <h3 id="long-term-potentiation">Long-Term Potentiation</h3> <p><br/></p> <p>Long-term potentiation (LTP) is a process involving persistent strengthening of synapses that leads to a long-lasting increase in signal transmission between neurons. It is an important process in the context of synaptic plasticity. LTP recording is widely recognized as a cellular model for the study of memory.</p> <p>LTP is like a marriage bond, connections are stronger between the presynpatic and postsynaptic neurons.</p> <p>Long Term Potentiation - the biomolecular process that your neurons go through as you learn</p> <ul> <li>how connections between neurons are strengthened through repeated pairing/firing</li> <li>neurons that fire together, wire together (using LTP)</li> </ul> <p>Our brain cells communicate with one another via synaptic transmission - one brain cell releases a chemical (neurotransmitter) that the next brain cell absorbs. This communication process is known as “neuronal firing.” When brain cells communicate frequently, the connection between them strengthens. Messages that travel the same pathway in the brain over &amp; over begin to transmit faster and faster. With enough repetition, they become automatic. That’s why we practice things like hitting a golf ball - with enough practice, we can go on automatic pilot. <br/><br/><br/> A dendritic spine is a small membranous profusion from a neuron’s dendrite that typically receives input from a single axon at the synapse.</p> <p>The more the spines are stimulated, the bigger, more stable, and longer lived they become (in contrast with smaller, immature spines, which are more transient). These large, so-called mushroom or stubby spines are thought to be one of the physical correlates of memory.</p> <figure> <picture> <img src="/assets/img/spine.jpg" width="60%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Repeated long-term potentiation causes spines to become larger and heartier. <br/><br/></p> <p>Synapses are strengthened and maintained in stages: Changes in synaptic strength that support LTP evolve in stages that can be identified by the unique molecular processes that support each stage.</p> <p>Lasting synaptic changes depend on a temporally ordered sequence of molecular events</p> <ul> <li>Generation, Stabilization, Consolidation, Maintenance</li> </ul> <p><br/></p> <p>To create permanent memories through this system, emphasize occasional activation of neural pathways. Occasional activation signals a host of maintenance molecular processes.</p>]]></content><author><name></name></author><category term="neurobiology"/><summary type="html"><![CDATA[interesting concepts from neurobiology]]></summary></entry><entry><title type="html">LLM optimizations</title><link href="https://michaelc-yu.github.io/blog/2024/optimizations/" rel="alternate" type="text/html" title="LLM optimizations"/><published>2024-10-26T23:36:10+00:00</published><updated>2024-10-26T23:36:10+00:00</updated><id>https://michaelc-yu.github.io/blog/2024/optimizations</id><content type="html" xml:base="https://michaelc-yu.github.io/blog/2024/optimizations/"><![CDATA[<h2 id="optimizing-the-attention-mechanism">Optimizing the attention mechanism</h2> <p><br/></p> <h4 id="multi-query-attention">Multi-query attention</h4> <p>Multi-query attention (MQA) shares the keys and values among all the attention heads. The query vector is still projected multiple times, as before, but there is one set of keys and values for all heads. While the amount of computation done in MQA is identical to multi-head attention, the amount of data (keys, values) read from memory is a fraction of before. When bound by memory-bandwidth, this enables better compute utilization. It also reduces the size of the KV-cache in memory, allowing space for larger batch sizes.</p> <h4 id="grouped-query-attention">Grouped-query attention</h4> <p>Grouped-query attention (GQA) projects keys and values to a few groups of query heads. It has more key-value heads than one, but fewer than the number of query heads. It’s a balance between multi-head attention and multi-query attention.</p> <figure> <picture> <img src="/assets/img/gqa.jpg" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><br/></p> <h4 id="flash-attention">Flash attention</h4> <p>We can also optimize the attention mechanism by reordering certain computations to make better use of the GPU’s memory hierarchy. Neural networks are usually organized by layers, and most implementations follow this structure, with each type of computation applied in sequence to the input data. However, this layer-by-layer approach isn’t always the most efficient. Often, it’s more effective to perform additional calculations on data that’s already loaded into the higher, faster levels of GPU memory.</p> <p>By combining multiple layers during computation, we can reduce the number of times the GPU has to read from and write to its memory. This also lets us group calculations that need the same data, even if they come from different layers in the network. This fusion of operations can lead to more efficient processing overall.</p> <p>FlashAttention is an I/O aware exact attention algorithm.</p> <p>I/O aware means it takes into account some of the memory movement costs previously discussed when fusing operations together. In particular, FlashAttention using “tiling” to fully compute and write out a small part of the final matrix at once, rather than doing part of the computation on the whole matrix in steps, writing out the intermediate values in between.</p> <p>Exact attention means that it is mathematically identical to the standard multi-head attention (with variants available for multi-query and grouped-query attention), and so can be swapped into an existing model architecture or even an already-trained model with no modifications.</p> <figure> <picture> <img src="/assets/img/flashattn.jpg" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> ]]></content><author><name></name></author><category term="machine-learning"/><summary type="html"><![CDATA[optimizing inference and memory in large language models]]></summary></entry><entry><title type="html">transformer architecture</title><link href="https://michaelc-yu.github.io/blog/2024/transformer/" rel="alternate" type="text/html" title="transformer architecture"/><published>2024-10-26T15:09:00+00:00</published><updated>2024-10-26T15:09:00+00:00</updated><id>https://michaelc-yu.github.io/blog/2024/transformer</id><content type="html" xml:base="https://michaelc-yu.github.io/blog/2024/transformer/"><![CDATA[<p>The transformer architecture is made up of an encoder block and a decoder block.</p> <p>The encoder’s role is to process the input sequence in its entirety and distill it into a condensed vector representation. The encoder consits of a series of N=6 identical layers, each containing two principle sub-layers: a multi-head self-attention mechanism and a position-wise fully connected feed-forward network.</p> <p>The decoder’s role is output generation. It does this sequentially (next token prediction). The decoder also consists of N=6 identical layers. However, it includes an additional third sub-layer that facilitates multi-head attention over the encoder’s output. <br/></p> <h4 id="encoder-in-depth">Encoder In-Depth</h4> <ul> <li>each word/token in the input sentence is converted into a dense vector using token embeddings</li> <li>positional encoding is added to each embedding to provide positional information. The Transformer architecture doesn’t account for sequence order, so this is necessary.</li> <li>each token embedding is linearly transformed into a query (Q), key (K), and value (V) vector through learned weight matrices.</li> <li>the model computes the dot product between each token’s query vector and each token’s key vector in the sequence to calculate attention scores. This score gives a similarity score that reflects how much attention the two tokens should give each other.</li> <li>we follow with a scaling and applying a softmax to these scores and then multiplying by the value vectors to get a weighted sum of all value vectors</li> <li>we then pass this output through a feed-forward neural network</li> <li>we stack 6 or more of these encoder layers together to form the entire encoder block <br/></li> </ul> <h4 id="decoder-in-depth">Decoder In-Depth</h4> <ul> <li>we also use token embeddings to convert the input tokens (which are the output tokens of the encoder so far) into a dense vector</li> <li>apply positional encoding</li> <li>a Self Attention Mechanism is applied with a masking mechanism to maintain that the decoder can only attend to earlier positions in the output sequence</li> <li>a second attention mechanism: now the Q vector comes from the decoder’s previous layer but K and V come from the encoder’s output. This attention mechanism allows the decoder to focus on relevant parts of the input sentence.</li> <li>the output of the encoder-decoder attention goes through a feed-forward network</li> <li>we stack 6 or more of these decoder layers together to form the entire decoder block <br/></li> </ul>]]></content><author><name></name></author><category term="machine-learning"/><summary type="html"><![CDATA[in-depth walkthrough of transformer architecture]]></summary></entry></feed>