<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://michaelc-yu.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://michaelc-yu.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-10-27T22:21:48-07:00</updated><id>https://michaelc-yu.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">transformer architecture</title><link href="https://michaelc-yu.github.io/blog/2024/transformer/" rel="alternate" type="text/html" title="transformer architecture"/><published>2024-10-26T08:09:00-07:00</published><updated>2024-10-26T08:09:00-07:00</updated><id>https://michaelc-yu.github.io/blog/2024/transformer</id><content type="html" xml:base="https://michaelc-yu.github.io/blog/2024/transformer/"><![CDATA[]]></content><author><name></name></author><category term="machine-learning"/><summary type="html"><![CDATA[in-depth walkthrough of transformer architecture]]></summary></entry><entry><title type="html">LLM optimizations</title><link href="https://michaelc-yu.github.io/blog/2024/optimizations/" rel="alternate" type="text/html" title="LLM optimizations"/><published>2024-04-29T16:36:10-07:00</published><updated>2024-04-29T16:36:10-07:00</updated><id>https://michaelc-yu.github.io/blog/2024/optimizations</id><content type="html" xml:base="https://michaelc-yu.github.io/blog/2024/optimizations/"><![CDATA[ <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;!--
  See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
  https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
--&gt;

&lt;img
  src="/assets/img/gqa.jpg"
  
  
    width="100%"
  
  
    height="auto"
  
  
  
  
  
  
    loading="eager"
  
  onerror="this.onerror=null; $('.responsive-img-srcset').remove();"
&gt;
</code></pre></div></div> <p>&lt;/picture&gt;</p> <p>&lt;/figure&gt;</p> <h3 id="flash-attention">Flash attention</h3> <p>Another way of optimizing the attention mechanism is to modify the ordering of certain computations to take better advantage of the memory hierarchy of GPUs. Neural networks are generally described in terms of layers, and most implementations are laid out that way as well, with one kind of computation done on the input data at a time in sequence. This doesn’t always lead to optimal performance, since it can be beneficial to do more calculations on values that have already been brought into the higher, more performant levels of the memory hierarchy.</p> <p>Fusing multiple layers together during the actual computation can enable minimizing the number of times the GPU needs to read from and write to its memory and to group together calculations that require the same data, even if they are parts of different layers in the neural network.</p> <p>One very popular fusion is FlashAttention, an I/O aware exact attention algorithm, as detailed in FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness. Exact attention means that it is mathematically identical to the standard multi-head attention (with variants available for multi-query and grouped-query attention), and so can be swapped into an existing model architecture or even an already-trained model with no modifications.</p> <p>I/O aware means it takes into account some of the memory movement costs previously discussed when fusing operations together. In particular, FlashAttention using “tiling” to fully compute and write out a small part of the final matrix at once, rather than doing part of the computation on the whole matrix in steps, writing out the intermediate values in between.</p> <figure> <picture> <img src="/assets/img/flashattn.jpg" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <hr/> <h2 id="modifications-to-model-weights">Modifications to model weights</h2> <h3 id="quantization">Quantization</h3> <p>Quantization is the process of reducing the precision of a model’s weights and activations. Most models are trained with 32 or 16 bits of precision, where each parameter and activation element takes up 32 or 16 bits of memory - a single-precision floating point. However, most deep learning models can be effectively represented with eight or even fewer bits per value.</p> <h3 id="sparsity">Sparsity</h3> <p>Similar to quantization, it’s been shown that many deep learning models are robust to pruning, or replacing certain values that are close to 0 with 0 itself. Sparse matrices are matrices where many of the elements are 0. These can be expressed in a condensed form that takes up less space than a full, dense matrix.</p> <h3 id="distillation">Distillation</h3> <p>This process involves training a smaller model that’s called a student model to mimic the behavior of the larger model (the teacher model). The student model will be trained to mirror the performance of the teacher model, with a loss function that measures the discrepancy between their outputs. DistilBERT compresses a BERT model by 40% while retaining 97% of its language understanding capabilities at a 60% faster speed.</p> <p>–&gt;</p>]]></content><author><name></name></author><category term="machine-learning"/><summary type="html"><![CDATA[walkthrough of various model optimization techniques]]></summary></entry><entry><title type="html">neuroscience notes</title><link href="https://michaelc-yu.github.io/blog/2024/neuroscience/" rel="alternate" type="text/html" title="neuroscience notes"/><published>2024-04-29T16:36:10-07:00</published><updated>2024-04-29T16:36:10-07:00</updated><id>https://michaelc-yu.github.io/blog/2024/neuroscience</id><content type="html" xml:base="https://michaelc-yu.github.io/blog/2024/neuroscience/"><![CDATA[]]></content><author><name></name></author><category term="neuroscience"/><summary type="html"><![CDATA[interesting concepts from neuroscience and psychobiology]]></summary></entry><entry><title type="html">decision-making</title><link href="https://michaelc-yu.github.io/blog/2024/decision/" rel="alternate" type="text/html" title="decision-making"/><published>2024-04-29T16:36:10-07:00</published><updated>2024-04-29T16:36:10-07:00</updated><id>https://michaelc-yu.github.io/blog/2024/decision</id><content type="html" xml:base="https://michaelc-yu.github.io/blog/2024/decision/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[how I approach decision-making under uncertainty]]></summary></entry><entry><title type="html">Benchling Company Analysis. In the last decade, the life sciences… | by Michael Yu | Medium</title><link href="https://michaelc-yu.github.io/blog/2023/benchling-company-analysis-in-the-last-decade-the-life-sciences-by-michael-yu-medium/" rel="alternate" type="text/html" title="Benchling Company Analysis. In the last decade, the life sciences… | by Michael Yu | Medium"/><published>2023-06-10T00:00:00-07:00</published><updated>2023-06-10T00:00:00-07:00</updated><id>https://michaelc-yu.github.io/blog/2023/benchling-company-analysis-in-the-last-decade-the-life-sciences--by-michael-yu--medium</id><content type="html" xml:base="https://michaelc-yu.github.io/blog/2023/benchling-company-analysis-in-the-last-decade-the-life-sciences-by-michael-yu-medium/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[In the last decade, the life sciences industry has experienced booming growth fueled by cutting-edge technologies such as single-cell sequencing, gene-editing systems like CRISPR-CAS9, and synthetic…]]></summary></entry><entry><title type="html">Displaying External Posts on Your al-folio Blog</title><link href="https://michaelc-yu.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog/" rel="alternate" type="text/html" title="Displaying External Posts on Your al-folio Blog"/><published>2022-04-23T16:20:09-07:00</published><updated>2022-04-23T16:20:09-07:00</updated><id>https://michaelc-yu.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog</id><content type="html" xml:base="https://michaelc-yu.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog/"><![CDATA[]]></content><author><name></name></author></entry><entry><title type="html">a post with pseudo code</title><link href="https://michaelc-yu.github.io/blog/2022/pseudocode/" rel="alternate" type="text/html" title="a post with pseudo code"/><published>2022-04-14T17:01:00-07:00</published><updated>2022-04-14T17:01:00-07:00</updated><id>https://michaelc-yu.github.io/blog/2022/pseudocode</id><content type="html" xml:base="https://michaelc-yu.github.io/blog/2022/pseudocode/"><![CDATA[]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="code"/><summary type="html"><![CDATA[this is what included pseudo code could look like]]></summary></entry><entry><title type="html">math / stats for ML</title><link href="https://michaelc-yu.github.io/blog/2015/math/" rel="alternate" type="text/html" title="math / stats for ML"/><published>2015-10-20T08:12:00-07:00</published><updated>2015-10-20T08:12:00-07:00</updated><id>https://michaelc-yu.github.io/blog/2015/math</id><content type="html" xml:base="https://michaelc-yu.github.io/blog/2015/math/"><![CDATA[]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="math"/><summary type="html"><![CDATA[an example of a blog post with some math]]></summary></entry><entry><title type="html">fine-tuning</title><link href="https://michaelc-yu.github.io/blog/2015/code/" rel="alternate" type="text/html" title="fine-tuning"/><published>2015-07-15T08:09:00-07:00</published><updated>2015-07-15T08:09:00-07:00</updated><id>https://michaelc-yu.github.io/blog/2015/code</id><content type="html" xml:base="https://michaelc-yu.github.io/blog/2015/code/"><![CDATA[]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="code"/><summary type="html"><![CDATA[an example of a blog post with some code]]></summary></entry><entry><title type="html">a post with images</title><link href="https://michaelc-yu.github.io/blog/2015/images/" rel="alternate" type="text/html" title="a post with images"/><published>2015-05-15T14:01:00-07:00</published><updated>2015-05-15T14:01:00-07:00</updated><id>https://michaelc-yu.github.io/blog/2015/images</id><content type="html" xml:base="https://michaelc-yu.github.io/blog/2015/images/"><![CDATA[ <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;!--
  See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
  https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
--&gt;

&lt;img
  src="/assets/img/9.jpg"
  
    class="img-fluid rounded z-depth-1"
  
  
    width="100%"
  
  
    height="auto"
  
  
  
  
  
  
    loading="eager"
  
  onerror="this.onerror=null; $('.responsive-img-srcset').remove();"
&gt;
</code></pre></div></div> <p>&lt;/picture&gt;</p> <p>&lt;/figure&gt;</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;/div&gt;
&lt;div class="col-sm mt-3 mt-md-0"&gt;
</code></pre></div></div> <figure> <picture> <img src="/assets/img/7.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;/div&gt;
</code></pre></div></div> <p>&lt;/div&gt;</p> <div class="caption"> A simple, elegant caption looks good between image rows, after each row, or doesn't have to be there at all. </div> <p>Images can be made zoomable. Simply add <code class="language-plaintext highlighter-rouge">data-zoomable</code> to <code class="language-plaintext highlighter-rouge">&lt;img&gt;</code> tags that you want to make zoomable.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/8.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/10.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>The rest of the images in this post are all zoomable, arranged into different mini-galleries.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/11.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/12.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/7.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>–&gt;</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="images"/><summary type="html"><![CDATA[this is what included images could look like]]></summary></entry></feed>