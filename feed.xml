<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://michaelc-yu.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://michaelc-yu.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-09-25T05:36:59+00:00</updated><id>https://michaelc-yu.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Engineering cheat-sheet to burn into my memory</title><link href="https://michaelc-yu.github.io/blog/2025/eng-cheat-sheet/" rel="alternate" type="text/html" title="Engineering cheat-sheet to burn into my memory"/><published>2025-09-03T12:00:00+00:00</published><updated>2025-09-03T12:00:00+00:00</updated><id>https://michaelc-yu.github.io/blog/2025/eng-cheat-sheet</id><content type="html" xml:base="https://michaelc-yu.github.io/blog/2025/eng-cheat-sheet/"><![CDATA[<h4 id="cuda--gpus"><b>CUDA / GPUs</b></h4> <p>Keep tensors on GPU and avoid .cpu() / .numpy() unless required</p> <p>Use mixed precision!</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>FP32 (single precision) -&gt; standard, stable
FP16/BF16 (mixed precision) -&gt; faster, less memory
INT8/FP8 -&gt; quantized inference, big speed/memory gains
</code></pre></div></div> <p>Free unused tensors:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">del</span> <span class="n">tensor</span><span class="p">;</span>
<span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">empty_cache</span><span class="p">()</span>
</code></pre></div></div> <p>PyTorch: torch.profiler, torch.cuda.synchronize() around timers</p> <p>Watch GPU utilization with nvidia-smi (if it’s low, it’s data-bound not compute-bound!)</p> <p><br/></p> <h4 id="curl"><b>Curl</b></h4> <p>Basics</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>curl https://my-api.com           <span class="c"># simple GET</span>
curl <span class="nt">-v</span> https://my-api.com        <span class="c"># verbose output (headers, etc.)</span>
curl <span class="nt">-I</span> https://my-api.com        <span class="c"># HEAD request (headers only)</span>
</code></pre></div></div> <p>HTTP Methods</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>curl <span class="nt">-X</span> GET https://my-api.com/resource
curl <span class="nt">-X</span> POST https://my-api.com/resource
curl <span class="nt">-X</span> PUT https://my-api.com/resource/123
curl <span class="nt">-X</span> DELETE https://my-api.com/resource/123
</code></pre></div></div> <p>Send Data</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>curl <span class="nt">-d</span> <span class="s2">"param1=value1&amp;param2=value2"</span> https://my-api.com/form
curl <span class="nt">-H</span> <span class="s2">"Content-Type: application/json"</span> <span class="se">\</span>
     <span class="nt">-d</span> <span class="s1">'{"id":"123","ttl":25}'</span> <span class="se">\</span>
     https://my-api.com/users
</code></pre></div></div> <p><br/></p> <h4 id="grpc"><b>gRPC</b></h4> <p>gRPC (Google Remote Procedure Call) = an open-source high-performance framework for building distributed systems and APIs.</p> <p>Uses Protobufs as default interface definition language and data serialization format.</p> <p>Client calls function inside proto file and gRPC handles serialization, transport (HTTP/2 by default), and deserialization.</p> <p>HTTP/2 = second major version of HTTP and supports multiplexing (allows multiple signals / data streams to be combined and transmitted simultaneously).</p> <p><br/></p> <h4 id="protobuf"><b>Protobuf</b></h4> <p>Protocol Buffers (Protobuf) is a language-neutral, platform-neutral, efficient mechanism for serializing structured data, developed by Google</p> <p>Serialization = turning structured data into bytes for storage / transmission</p> <p>Deserialization = turning those bytes back into usable objects</p> <p>Define data schema in a .proto file: <br/></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>syntax = "proto3";

message Person {
  string name = 1;
  int32 id = 2;
  string email = 3;
}
</code></pre></div></div> <p>Compile with <code class="language-plaintext highlighter-rouge">protoc</code></p> <p>Default serialization format in gRPC APIs</p> <p><br/></p> <h4 id="aws-s3-cli"><b>AWS s3 CLI</b></h4> <p>Upload a file:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>aws s3 <span class="nb">cp </span>file.txt s3://my-bucket-name/
</code></pre></div></div> <p>Upload a directory (recursive)</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>aws s3 <span class="nb">cp </span>myfolder/ s3://my-bucket-name/ <span class="nt">--recursive</span>
</code></pre></div></div> <p>Download a file</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>aws s3 <span class="nb">cp </span>s3://my-bucket-name/file.txt ./localfile.txt
</code></pre></div></div> <p>Sync local &lt;-&gt; S3</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>aws s3 <span class="nb">sync</span> ./localfolder s3://my-bucket-name/folder
aws s3 <span class="nb">sync </span>s3://my-bucket-name/folder ./localfolder
</code></pre></div></div> <p><br/></p>]]></content><author><name></name></author><category term="systems;"/><category term="engineering"/><summary type="html"><![CDATA[CUDA / GPUs]]></summary></entry><entry><title type="html">Recsys Models</title><link href="https://michaelc-yu.github.io/blog/2025/recsys-models/" rel="alternate" type="text/html" title="Recsys Models"/><published>2025-07-20T00:36:00+00:00</published><updated>2025-07-20T00:36:00+00:00</updated><id>https://michaelc-yu.github.io/blog/2025/recsys-models</id><content type="html" xml:base="https://michaelc-yu.github.io/blog/2025/recsys-models/"><![CDATA[<p>List of common and popular models and algorithms used in recommendation systems.</p> <h2 id="sequential-recommendation"><b>Sequential Recommendation</b></h2> <h4 id="gru4rec"><b>GRU4Rec</b></h4> <p>Many e-commerce recommender systems or news and media sites don’t track user behavior across different sessions for privacy concerns and because it’s difficult and unreliable with cookies and browser fingerprinting. Introduce session-based recommendation, where you don’t know the user’s long-term profile — you only observe the current session. GRU4Rec applies the GRU-based RNN for session-based recommendations. The model tries to capture short-term intent within the session.</p> <h4 id="sasrec"><b>SASRec</b></h4> <p>Self-Attentive Sequential Recommendation. Proposed in 2018 and one of the first works to apply the Transformer architecture (self-attention) to the sequential recommendation problem. It formulates sequential recommendation as next-item prediction, given a user’s past interaction sequence. Self-attention helps capture both short and long-term preferences much better and more parallelizable than previous approaches like RNNs or Markov chains. It outperforms GRU4Rec on several datasets including MovieLens. It uses only the Transformer encoder stack. SASRec can process the entire concatenated history of a user — including multiple sessions — and predict the next item.</p> <p><br/><br/></p> <h2 id="multi-armed-bandits--exploration-exploitation"><b>Multi-Armed Bandits / Exploration-Exploitation</b></h2> <p>Multi-armed bandits (bandits) are a type of reinforcement learning. Their advantages over batch machine learning and A/B testing methods include continuously learning and adapting recommendations without the need for extensive data collection or offline model training. Usually performs better in situations with limited data or in cold-start scenarios.</p> <h4 id="epsilon-greedy"><b>Epsilon-Greedy</b></h4> <p>With probability ϵ, you pick a random arm (explore), and with probability 1 - ϵ, you pick the arm with the highest estimated mean reward (exploit). Note: this is not a contextual bandit approach (you don’t observe the context for each decision). ϵ parameter controls the balance between exploration and exploitation. A higher ϵ means more exploration, while a lower ϵ means more exploitation. A strategy is to decrease the value of ϵ over time and allow for more exploitation.</p> <h4 id="linucb"><b>LinUCB</b></h4> <p>These algorithms, in each trial, estimate both the mean payoff of each arm as well as a corresponding confidence interval. They then select the arm that achieves the highest upper confidence bound.<br/></p> \[\begin{array}{l} \textbf{For each arm } a: \\ \quad A_a \leftarrow I_d \\ \quad b_a \leftarrow 0_d \\[1em] \textbf{For each round } t: \\ \quad \text{Observe context vector } x_{a,t} \text{ for all arms } a \\[0.5em] \quad \textbf{For each arm } a: \\ \quad\quad \hat{\theta}_a \leftarrow A_a^{-1} b_a \\ \quad\quad \text{mean_reward}_a \leftarrow x_{a,t}^\top \hat{\theta}_a \\ \quad\quad \text{uncertainty}_a \leftarrow \sqrt{x_{a,t}^\top A_a^{-1} x_{a,t}} \\ \quad\quad p_a \leftarrow \text{mean_reward}_a + \alpha \cdot \text{uncertainty}_a \\[1em] \quad \text{Choose arm } a_t \leftarrow \arg\max_a p_a \\[0.5em] \quad \text{Observe reward } r_t \\[0.5em] \quad \textbf{Update:} \\ \quad\quad A_{a_t} \leftarrow A_{a_t} + x_{a_t} x_{a_t}^\top \\ \quad\quad b_{a_t} \leftarrow b_{a_t} + r_t x_{a_t} \\ \end{array}\] <p>Notes:<br/> -we assume each arm has its own independent linear model<br/> -pₐ is the upper confidence bound for arm a at time t<br/> -\(\alpha\) is the exploration hyperparameter<br/> -\(x_{a,t}\) is the context vector given by the problem, per arm, per round. Ex. Arms = articles. At time t, each article has context vector describing user profile, article topic, recency, etc.<br/> -\(x_{a,t}^\top \hat{\theta}_a\) is best estimate of expected reward for arm a<br/> -\(\sqrt{x_{a,t}^\top A_a^{-1} x_{a,t}}\) is how uncertain we are about this estimate (standard error)<br/></p> <h4 id="thompson-sampling"><b>Thompson Sampling</b></h4> <p>Thompson Sampling is a Bayesian algorithm. In each round, parameters are sampled from the posterior and an action is chosen that maximizes expected reward given the sampled parameters and current context. Observe the reward and update the posterior for the arm that was just chosen.</p> \[\begin{aligned} &amp;\text{For each arm } a \in \{1, \dots, K\}, \text{ assume:} \\ &amp;\quad \theta_a \sim \text{Beta}(\alpha_a, \beta_a) \\ &amp;\quad r_t \sim \text{Bernoulli}(\theta_{a_t}) \\\\ &amp;\text{At each time step } t: \\ &amp;\quad \text{Sample } \hat{\theta}_a \sim \text{Beta}(\alpha_a, \beta_a), \quad \forall a \in \{1, \dots, K\} \\ &amp;\quad \text{Select arm } a_t = \arg\max_a \hat{\theta}_a \\ &amp;\quad \text{Play arm } a_t \text{ and observe reward } r_t \in \{0, 1\} \\\\ &amp;\text{Update the posterior:} \\ &amp;\quad \alpha_{a_t} \leftarrow \alpha_{a_t} + r_t \\ &amp;\quad \beta_{a_t} \leftarrow \beta_{a_t} + (1 - r_t) \end{aligned}\] <p><br/><br/></p> <h2 id="wide--deep-architectures"><b>Wide &amp; Deep Architectures</b></h2> <h4 id="wide--deep"><b>Wide &amp; Deep</b></h4> <p>Wide &amp; Deep learning trains wide linear models and deep neural networks together to combine the benefits of memorization (wide part) and generalization (deep part) for recommender systems.<br/> The wide component’s feature crosses are interpretable and effective and capturing known, explicit relationships. However, it also requires manual feature engineering to create effective crossing of features. For example (gender x city) or (item category x user membership).<br/></p> <p>The deep component is good at learning complex patterns. No manual feature engineering is required here since the model automatically learns through non-linear activation functions.<br/></p> <p>The output of both wide and deep components can be either scalar (solving binary classification problem like predicting CTR) or a vector of logits (solving a multi-class classification problem). Outputs of both components are combined to make final predictions. Can combine through either summing or concatenating and then feeding through a final dense layer.</p> <h4 id="deepfm"><b>DeepFM</b></h4> <p>Combines factorization machines (FM) for feature interactions (memorizations) and deep neural networks for generalization. Similar to Wide &amp; Deep, but with factorization machines you don’t need to manually craft cross features since FM does that for you. So it’s just like Wide &amp; Deep with ability to memorize and generalize patterns but wide part is implemented with factorization machine.<br/><br/> Factorization machine formula:<br/><br/> \(\hat{y}(x) = w_0 + \sum_{i=1}^{n} w_i x_i + \sum_{i=1}^{n} \sum_{j=i+1}^{n} \langle \mathbf{v}_i, \mathbf{v}_j \rangle x_i x_j\) <br/><br/> -\(x_i\) are the input features<br/> -\(w_0\) is a global bias<br/> -\(\mathbf{v}_i\) is an embedding vector for feature i<br/> <br/> FMs generalize matrix factorization to work with any kind of feature data, not just user and item IDs. FMs give pairwise interactions.</p> <p><br/><br/></p> <h2 id="classic-methods"><b>Classic Methods</b></h2> <h4 id="matrix-factorization-mf"><b>Matrix Factorization (MF)</b></h4> <p>Simple embedding model that decomposes a sparse user-item feedback matrix into the product of two dense lower-dimensional matrices. One is the user embedding matrix and the other is the item embedding matrix. Their product approximates the sparse rating matrix. This way, we can learn embedding for users and items.</p> <h4 id="neural-collaborative-filtering-ncf"><b>Neural Collaborative Filtering (NCF)</b></h4> <p>Replaces the dot product in matrix factorization with neural networks. We concatenate user and item vector embeddings and feed into MLP which predicts something like click or no click, rating, etc. We train on interaction data; the user and item embeddings are learned during training and they are initially random vectors. Using neural networks instead of dot product (as in matrix factorization) allows nonlinearity and ability to model more complex patterns.</p> <p><br/><br/></p> <h2 id="graph-based-methods"><b>Graph-Based Methods</b></h2> <h4 id="gcn-based-recsys"><b>GCN-based RecSys</b></h4> <p>Graph Convolutional Networks learn from data where the relationships between elements are represented as a graph with nodes and edges. A GCN layer updates each node’s representation by aggregating features from its neighbors:</p> <p>-message passing: each node gathers information from its neighbors</p> <p>-aggregation: the collected features are combined (usually via summation, mean, or max)</p> <p>-transformation: the aggregated information is passed through a neural network layer (e.g., a linear transformation + activation)</p> <p>Stacking multiple convolutions allows information flow across far reaches of graph</p> <p><br/></p> <h4 id="pinsage"><b>PinSage</b></h4> <p>PinSage, released in 2018, is a highly-scalable (operating on a massive graph with three billion nodes and 18 billion edges) graph convolutional network framework developed and deployed at Pinterest.</p> <p>While previous GCN approaches use k-hop graph neighborhoods, PinSage defines neighborhoods based on importance of nearby nodes. The neighborhoods of a node are the T nodes that exert the most influence on that node.</p> <h5 id="key-points-about-pinsage-"><b>Key points about PinSage: </b></h5> <p>-simulate random walks starting from node u and compute the L1-normalized visit count of nodes visited by the random walk. The neighborhood of u is then defined as the top T nodes with the highest normalized visit counts with respect to node u</p> <p>-stacking multiple convolutions on top of each other to gain more information about the local graph structure around node u. Layer 0 representations are equal to the input node features, and all other layers depend on the output from the previous layer</p> <p>-train PinSage using max-margin ranking loss: maximize inner product of positive pairs (q, i) (item i is a good recommendation candidate for query q), and minimize inner product of negative pairs, such that inner product of negative examples is smaller than that of positive examples by some pre-defined margin</p> <p>-train with multiple GPUs via data parallelism and large batch sizes</p> <p>-use re-indexing technique to create a sub-graph G’ = (V’, E’) containing nodes and their neighborhood which will be involved in the computation of the current minibatch. Otherwise GPU accsesing data in CPU for neighborhood and feature information is slow and inefficient</p> <p>-negative sampling: random selection of some negative pairs to push them apart when training. In PinSage training, they sample a set of 500 negative items that’s shared by all training examples in each minibatch to speed up training. Also uses hard negative examples, which are somewhat related items but not as related as positive pair, to make model learn at finer granularity</p> <p>-since PinSage itself doesn’t compute embeddings (it only trains a GCN to generate an embedding function given a node’s features and neighborhood information), it needs an efficient way to actually compute the embedding vectors. It does so via MapReduce</p> <p>-given a query q, we can directly recommend items whose embeddings are the K-nearest neighbors of that embedding, based on the embeddings computed by PinSage. Approximate KNN can be obtained efficiently via locality sensitive hashing. See here: <a href="https://en.wikipedia.org/wiki/Locality-sensitive_hashing">Locality sensitive hashing wikipedia</a></p>]]></content><author><name></name></author><category term="machine-learning"/><summary type="html"><![CDATA[Recsys models & algorithms]]></summary></entry><entry><title type="html">Recommendation Systems Notes</title><link href="https://michaelc-yu.github.io/blog/2025/recsys/" rel="alternate" type="text/html" title="Recommendation Systems Notes"/><published>2025-05-14T13:36:11+00:00</published><updated>2025-05-14T13:36:11+00:00</updated><id>https://michaelc-yu.github.io/blog/2025/recsys</id><content type="html" xml:base="https://michaelc-yu.github.io/blog/2025/recsys/"><![CDATA[<h4 id="introduction">Introduction</h4> <p><br/></p> <p>Recommendation systems utilize machine learning models to determine user preferences based on their past history or by analyzing the preferences of similar users. These systems typically involve documents (entities a system recommends, like movies or videos), queries (information needed to make recommendations, such as user data or location), and embeddings (mappings of queries or documents to a vector space called embedding space).</p> <p>Candidate Generation: the process begins here, starting with a vast corpus and generating a smaller subset of candidates Scoring: in this step, the model ranks the candidates to select a smaller, more refined set of documents Re-ranking: this final step refines the recommendations</p> <p>Many large-scale recommendation systems implement these processes in two main stages: retrieval and ranking</p> <p><br/><br/></p> <h5 id="2-stage-recommender-systems">2-Stage Recommender Systems</h5> <p><br/> Retrieval: Quickly narrows down the vast candidate pool (millions of items) to a smaller subset using scalable models such as matrix factorization or two-tower architectures Ranking: Applies sophisticated models incorporating richer feature sets, such as user context or item-specific details, to assign relevance scores and generate a personalized, ranked list</p> <p><br/></p> <h5 id="4-stage-recommender-systems">4-Stage Recommender Systems</h5> <p><br/> Retrieval: identifies an initial set of candidates, emphasizing efficiency and broad coverage Filtering: applies business rules to exclude ineligible or irrelevant items (out-of-stock products or regionally restricted content) Scoring: uses advanced algorithms, such as deep learning models, to predict interaction likelihood by analyzing user-item interactions and contextual features Ranking (or Ordering): finalizes the recommendation list, balancing relevance, diversity, novelty, and business goals</p> <p>Eugene Yan’s 2-Stage (2x2) Model of a Recommender System Eugene Yan’s 2x2 model identifies two main dimensions within discovery systems: -Environment: offline vs. online -Process: candidate retrieval vs. ranking</p> <p>-the offline stage flows bottom-up, producing the necessary artifacts for the online environment, while the online stage processes requests left-to-right, following retrieval and ranking steps to return a final set of recommendations or search results</p> <p><br/></p> <h5 id="offline-environment">Offline Environment</h5> <p><br/> -in the offline environment, batch processes handle tasks such as model training, creating embeddings for catalog items, and developing structures like approximate nearest neighbors (ANN) indices or knowledge graphs to identify item similarity. The offline stage may also involve loading item and user data into a feature store, which helps augment input data during ranking</p> <p><br/></p> <h5 id="online-environment">Online Environment</h5> <p><br/> -the online environment serves individual user requests in real-time, utilizing artifacts generated offline (ANN indices, knowledge graphs, models). Here, input items or queries are transformed into embeddings, followed by two main steps: Candidate Retrieval: quickly narrows down millions of items to a more manageable set of candidates, trading precision for speed Ranking: ranks the reduced set of items by relevance. This stage enables the inclusion of additional features such as item and user data or contextual information, which are computationally intensive but feasible due to the smaller candidate set</p> <p><br/><br/></p> <h5 id="candidate-generation">Candidate Generation</h5> <p><br/> Candidate generation is the first stage of recommendations and is typically achieved by finding features for users that relate to features of the items.</p> <p>It’s the process of selecting a set of items that are likely to be recommended to a user. This process is typically performed after user preferences have been collected, and it involves filtering a large set of potential recommendations down to a more manageable number. The goal of candidate generation is to reduce the number of items that must be evaluated by the system, while still ensuring that the most relevant recommendations are included.</p> <p>Given a query (user info), the model generates a set of relevant candidates (videos, movies).</p> <p>There are two common candidate generation approaches: <br/> Content-based filtering: uses similarity between content to recommend new content if user watches corgi videos, the model will recommend more corgi videos <br/> Collaborative filtering: uses similarity between queries (2 or more users) and items (videos, movies) to provide recommendations if user A watches corgi videos and user A is similar to user B (in demographics and other areas), then the model can recommend corgi videos to user B even if user B has never watched a corgi video before</p> <p><br/><br/></p> <h5 id="sparse-and-dense-features">Sparse and Dense Features</h5> <p><br/> Recommender systems typically deal with two kinds of features: dense and sparse. Dense features are continuous real values, such as movie ratings or release years. Sparse features, on the other hand, are categorical and can vary in cardinality, like movie genres or the list of actors in a film. Dense features are typically the continuous variables and low dimensionality categorical variables: age, gender, number of likes in the past week, etc. Sparse features are the high dimensionality categorical variables and they are represented by an embedding: user id, page id, ad id, etc.</p> <p><br/><br/></p> <h5 id="content-based-filtering">Content-Based Filtering</h5> <p><br/> Content-Based Filtering is a sophisticated recommendation system that suggests items to users by analyzing the similarity between item features and the user’s known preferences. This approach recommends items that resemble those a user has previously liked or interacted with.</p> <p>Content-based filtering offers several advantages over alternative recommendation approaches: Effective in Data-Limited Environments: this method is particularly useful in situations where there is limited or no data on user behavior, such as in new or niche domains with a small user base Personalized Recommendations: since content-based filtering relies on an individual user’s preferences rather than the collective behavior of other users, it can provide highly personalized recommendations Scalability: the model does not require data from other users, focusing solely on the current user’s information, which makes it easier to scale Recommendation of Niche Items: it can recommend niche items tailored to each user’s unique preferences, items that might not be of interest to a broader audience Ability to Recommend New Items: the method can recommend newly introduced items without waiting for user interaction data, as the recommendations are based on the item’s inherent features Capturing Unique User Interests: content-based filtering excels at capturing and catering to the distinct interests of users by recommending items based on their previous engagements</p> <p>Limitations of Content-Based Filtering <br/> Need for domain knowledge: features must often be manually engineered. Consequently, the effectiveness of the model is closely tied to the quality of these hand-engineered features Limited exploration of new interests: the model tends to recommend items within the user’s existing preferences, limiting its ability to introduce new or diverse interests Difficulty in discovering new interests <br/> Dependence on feature availability: method may be ineffective in situations where there is a lack of detailed item information or where items possess limited attributes or features</p> <p><br/><br/></p> <h5 id="collaborative-filtering">Collaborative Filtering</h5> <p><br/> A recommendation system technique used to suggest items to users based on the behaviors and preferences of a broader user group. It operates on the principle that users with similar tastes are likely to enjoy similar items.</p> <p>Unlike content-based filtering, collaborative filtering automatically learns embeddings without relying on manually engineered features.</p> <p>Advantages of collaborative filtering <br/> -Effectiveness in sparse data environments: collaborative filtering is particularly effective when there is limited information about item attributes, as it relies on user behavior rather than item features <br/> -Scalability: it performs well in environments with a large and diverse user base, making it suitable for systems with extensive datasets <br/> -Serendipitous Recommendations: the system can provide unexpected yet relevant recommendations, helping users discover items they might not have found independently <br/> -No Domain Knowledge Required: it does not require specific knowledge of item features, eliminating the need for domain-specific expertise in engineering these features <br/> -Efficiency: collaborative filtering models are generally faster and less computationally intensive than content-based filtering, as they do not require analysis of item-specific attributes</p> <p><br/></p> <p>Disadvantages of collaborative filtering <br/> -Data scarcity: it can struggle in situations where there is insufficient data on user behavior, limiting its effectiveness <br/> -Unique preferences: the system may have difficulty making accurate recommendations for users with highly unique or niche preferences, as it relies on similarities between users <br/> -Cold start problem: new items or users with limited data pose challenges for generating accurate recommendations due to the lack of historical interaction data <br/> -Difficulty handling niche interests</p> <p><br/><br/></p> <h5 id="matrix-factorization-mf">Matrix Factorization (MF)</h5> <p><br/> -Matrix Factorization (MF) is a simple embedding model. The algorithm performs a decomposition of the (sparse) user-item feedback matrix into the product of two (dense) lower-dimensional matrices. One matrix represents the user embeddings, while the other represents the item embeddings. In essence, the model learns to map each user to an embedding vector and similarly maps each item to an embedding vector, such that the distance between these vectors reflects their relevance.</p> <p>-as part of training, we aim to produce user and item embedding matrices so that their product is a good approximation of the feedback matrix. Aka when you multiply the two matrices you get a good approximation of the sparse matrix back.</p> <p>It takes a large, sparse user-item interaction matrix (R) and factorizes it into two lower-dimensional matrices.</p> <p>You multiply the full matrices to get the entire predicted matrix, but each individual prediction is a dot product between a user embedding and an item embedding. The dot product between the u-th user vector and the i-th item vector. It involves matrix multiplication globally, but it’s made up of dot products between user and item embeddings at the individual prediction level.</p> <p>Why do this? <br/> Fill in missing values: <br/> -the original matrix is super sparse (most users haven’t rated most items) <br/> -after factorization, the dot product between user and item embeddings gives a predicted rating or interaction score <br/> Learn latent structure: <br/> -learns hidden patterns behind why users like certain items <br/> Enable recommendations <br/> -once you have embeddings for all users and items, you can recommend top-k items, find similar users/items via vector similarity</p> <p><br/><br/></p> <h5 id="neural-collaborative-filtering">Neural Collaborative Filtering</h5> <p><br/> A type of recommendation system that uses neural networks to learn user-item interactions and predict preferences. It generalizes matrix factorization, a common technique in collaborative filtering, by replacing the dot product with a neural network architecture. This allows it to capture more complex and non-linear relationships between users and items. It takes a user ID and item ID and their embeddings. It concatenates the user and item embeddings and then feeds that through a MLP. The output is the predicted interaction score (e.g., click probability).</p> <p><br/><br/></p> <h5 id="implicit-vs-explicit-feedback-in-recsys">Implicit vs. Explicit Feedback in RecSys</h5> <p><br/> Explicit feedback is users directly give ratings (e.g. 4 stars, thumbs up). Data is clear, but sparser. <br/> Implicit feedback is inferred from behavior (e.g. clicks, watch time, purchases, skips). Much more abundant, but noisy—just because someone watched something doesn’t mean they liked it.</p> <p><br/><br/></p> <h5 id="deep-neural-network-based-recommendations">Deep Neural Network Based Recommendations</h5> <p><br/> -more ability to learn complex patterns in user behavior and item attributes. <br/> -One common approach for DNN-based recommendation is to use a matrix factorization model as a baseline and then incorporate additional layers of neural networks to capture more complex patterns in the user-item interactions <br/> -Another popular approach for DNN-based recommendation is to use a sequence modeling architecture, such as a RNN or a transformer network. These models can capture temporal dependencies in user behavior and item popularity, allowing for more accurate and personalized recommendations. For example, an RNN can be used to model the sequence of items that a user has interacted with over time, and then use this information to predict which item the user is likely to interact with next</p> <p><br/><br/></p> <h5 id="two-tower-model">Two-tower Model</h5> <p><br/> -many online platforms, like yt, facebook, and tiktok, use the two-tower model in their recommender system <br/> -the two-tower model consists of two sub-neural networks: query and item <br/> -the query tower encodes user data; the item tower encodes product data <br/> -the output of each tower is an embedding, i.e., a dense vector <br/> -the similarity of a user and product pair is measured using the dot product <br/> -the trained embeddings of query and item towers are stored for fast retrieval <br/> In a two-tower architecture, there are two towers or columns: one tower represents the user and the other tower represents the item. Each tower consists of multiple layers of neurons, which can be fully connected or sparse. The user tower takes as input the user’s features, such as demographic information, browsing history, or social network connections, and processes them through the layers to produce a user embedding, which is a low-dimensional vector representation of the user. The item tower takes as input the item’s features, such as its genre, cast, or director, and processes them through the layers to produce an item embedding, which is a low-dimensional vector representation of the item. The user and item embeddings are then combined using a similarity function, such as dot product or cosine similarity, to produce a score that represents the predicted rating or likelihood of interaction between the user and the item.</p> <p><br/><br/></p> <h5 id="candidate-retrieval">Candidate Retrieval</h5> <p><br/> -Now that you have an embedding model, how would you decide which items to recommend given a user? <br/> -at serve time, given a query, you start by doing one of the following: <br/> -for a matrix factorization model, the query (or user) embedding is known statically, and the system can simply look it up from the user embedding matrix <br/> -for a DNN model, the system computes the query embedding at serve time by running the network on the feature vector x <br/> -once you have the query embedding q, search for item embeddings that are close to q in the embedding space. This is a nearest neighbor problem. <br/></p> <p>Large-scale Retrieval <br/> -to compute the nearest neighbors in the embedding space, the system can exhaustively score every potential candidate. Exhausting scoring can be expensive for very large corpora, but you can use either of the following strategies to make it more efficient: -if the query embedding is known statically, the system can perform exhaustive scoring offline, pre-computing and storing a list of the top candidates for each query. This is a common practice for related-item recommendation -use approximate nearest neighbors</p> <p><br/><br/></p> <h4 id="metrics">Metrics</h4> <p><br/></p> <h5 id="click-through-rate-ctr">Click-Through Rate (CTR)</h5> <p><br/> Out of all the times we showed a particular item (or group of items), how often did users click on it?</p> <p><br/></p> <h5 id="ndcg-normalized-discounted-cumulative-gain">NDCG (Normalized Discounted Cumulative Gain)</h5> <p><br/> NDCG measures how well your ranking model orders items, taking into account both relevance and position in the list. The idea: Relevant items at the top of the list should get more credit than relevant items buried at the bottom. NDCG Evaluates your model’s ordering, not just which items are present</p> <p><br/></p> <h5 id="recallk">Recall@K</h5> <p><br/> Out of all the relevant items for this user, how many did my system successfully surface in the top K ranked list?</p> <p>In practice relevance is defined from logs</p> <p>Let’s say you want to compute recall@10 offline. You could define “relevant” items as: <br/> -items the user clicked in the past 24 hours <br/> -items they watched for &gt;30s in the past week <br/> -items they gave high ratings to in past data <br/> Then you ask: “Did our top-10 recommendation list contain any of these?”</p> <p><br/></p> <h5 id="map-mean-average-precision">MAP (Mean Average Precision)</h5> <p><br/> Measures how well the recommendation system ranks relevant items near the top of the recommendation list across all users. It’s useful when you want to reward systems that rank relevant items higher. You care not just what items are recommended, but where in the list they appear.</p> <p><br/></p> <h5 id="ndcg-vs-map-intuition">NDCG vs. MAP intuition:</h5> <p><br/> -NDCG is how well did you rank the most important items near the top? <br/> -MAP is how many of your top-ranked items were correct, and how early did you get them?</p> <p><br/><br/></p> <h5 id="other-stuff">Other stuff</h5> <p><br/> Diversity is usually injected during re-ranking, not the initial ranking stage</p> <p>Genre constraint: “no more than 2 items of the same genre in the top 5”</p> <p>Dot product is efficient and works well when user and item embeddings live in the same space and relevance is a function of similarity. But in cases where preferences are more conditional or nonlinear, like job or dating recommendations, we’d use deep interaction models instead to model richer user-item relationships.</p> <p>Popularity Bias: popular items get recommended more often. Less popular items are overlooked even if some users would love them. System tends to reinforce already popular content (rich get richer).</p> <p>Exposure Bias: you can’t recommend what users have never been shown. Just because a user didn’t interact with something doesn’t mean they disliked it — maybe they just never saw it.</p> <p>Both popularity and exposure bias lead to feedback loops and reduced discovery.</p> <p>Bandits are designed to maximize reward over time — but if not properly regularized, they can actually become greedy over a few high-reward arms, which reduces diversity. If the bandit finds that a few items consistently perform well, it might just keep recommending those over and over. Result is that popular items get even more exposure, while other potentially good items never get a chance.</p> <p><br/><br/></p> <h5 id="ranking--scoring">Ranking / Scoring</h5> <p><br/> Scoring refers to the process of assigning a score or a rating to each item in the candidate pool based on its similarity to the user’s preferences or past behavior. The scoring function is used to determine the relevance of each item to the user, and items with higher scores are considered to be more relevant.</p> <p>Ranking, on the other hand, is the process of ordering the items based on their scores. The items with the highest scores are ranked at the top of the recommendation list, while the items with the lowest scores are ranked at the bottom. The ranking process ensures that the most relevant items are presented to the user first.</p> <p>Let’s look at different methods and techniques used for scoring.</p> <p><br/><br/></p> <h5 id="re-ranking">Re-ranking</h5> <p><br/> In the final phase of a recommendation system, the system has the capability to re-rank candidates by taking into account additional criteria or constraints that were not initially considered. This re-ranking stage allows the system to assess factors beyond the initial scoring, such as the diversity of items within the recommended set.</p> <p><br/><br/></p> <h5 id="popular-architectures">Popular Architectures</h5> <p><br/> The wide and deep architecture is a model architecture that combines a linear model (wide) and a deep neural network (deep), and trains them together to make predictions.</p> <p>The “Wide” Part = memorization -think of it like manual features + sparse cross-products -implemented via logistic regression or linear models with sparse features</p> <p>The “Deep” Part = generalization -a deep neural network that takes in embeddings of user, item, and context -learns nonlinear patterns and abstract interactions -helps generalize to unseen combinations of features</p> <p>The wide model helps memorize known patterns (co-viewed items, past behaviors) The deep model helps predict and generalize to new behaviors During training, both are optimized together — the outputs are combined (via sum or weighted average)</p> <p>Latent factor models embed users and items into a shared vector space, where their similarity reflects the likelihood of interaction. They’re commonly trained using matrix factorization and are powerful for collaborative filtering, especially when explicit user-item interaction data is available</p> <p>Knowledge graphs in recommender systems represent entities like users, items, genres, actors, or creators as nodes, and their relationships (e.g., “acted in,” “belongs to genre,” “watched by”) as edges in a graph structure. This enables the model to capture rich, structured, and explainable connections beyond simple collaborative filtering. By traversing multi-hop paths (e.g., user → watched movie A → has actor X → acted in movie B), the system can recommend semantically relevant items, even in cold-start scenarios where historical interaction data is sparse. Knowledge graphs improve personalization, support content diversity, and enhance explainability, and are often used alongside traditional models or embedded into neural architectures like graph neural networks (GNNs) or meta-path based embeddings to power more intelligent and context-aware recommendations.</p> <p>Pointwise ranking predicts absolute engagement scores for each item independently, using losses like cross-entropy or regression. Pairwise ranking, on the other hand, models the relative preference between item pairs — optimizing for which item should rank higher — and is often trained with BPR loss. Pairwise is more directly aligned with ranking quality, but pointwise is simpler and commonly used in production.</p> <p><br/><br/></p> <h5 id="multi-armed-bandits">Multi-armed Bandits</h5> <p><br/> Imagine you have several slot machines (or “arms”) and each one gives a different reward when pulled. The goal is to maximize the total reward by pulling the most rewarding arms. However, you don’t know beforehand which arm is the best. Exploration vs. Exploitation: you need to balance Exploration: trying out different arms to learn which one gives the best reward Exploitation: pulling the arm that seems to give the best reward based on what you’ve learned so far</p> <p>Bandits offer advantages over batch machine learning and A/B testing methods, as they minimize regret by continuously learning and adapting recommendations without the need for extensive data collection or waiting for test results. They can provide better performance in situations with limited data or when dealing with long-tail or cold-start scenarios, where batch recommenders may favor popular items over potentially relevant but lesser-known options.</p> <p><br/><br/></p> <h5 id="contextual-bandits">Contextual Bandits</h5> <p><br/> Contextual bandits extend the multi-armed bandit problem by introducing context into the decision process. Rather than just choosing an arm based on past reward alone, contextual bandits take into account the current situation or “context” before making a decision. Context: the “context” refers to additional information that can help in decision-making. For example, in a news website, the context might include user information such as location, browsing history, or device type. Action: based on the context, the algorithm decides which action to take (which arm to pull, or which recommendation to make) Reward: after the action, the algorithm observes the reward (whether the user clicked the recommended item or not) and uses this feedback to improve future decisions</p> <p>Contextual Bandits vs. Full Reinforcement Learning <br/> -contextual bandits are a simplification of full reinforcement learning. In full RL, the agent needs to learn from sequences of actions and long-term rewards (i.e., the environment’s state changes over time). In contrast, contextual bandits only deal with one-time, immediate rewards and do not require learning long-term strategies. This makes them computationally simpler and ideal for situations where immediate feedback is available, like in recommender systems.</p> <p>Online learning <br/> Contextual bandits are highly suited for online learning, meaning they can adapt to user behavior in real-time. The system can continuously refine its recommendations as it gathers more feedback from the user, making it particularly effective in dynamic environments.</p> <p><br/><br/></p> <h4 id="system-design-for-recommendations-and-search">System Design for Recommendations and Search</h4> <p><br/></p> <p>The offline environment largely hosts batch processes such as model training (e.g., representation learning, ranking), creating embeddings for catalog items, and building an approximate nearest neighbors (ANN) index or knowledge graph to find similar items. It may also include loading item and user data into a feature store that is used to augment input data during ranking.</p> <p>The online environment then uses the artifacts generated (e.g., ANN indices, knowledge graphs, models, feature stores) to serve individual requests. A typical approach is converting the input item or search query into an embedding, followed by candidate retrieval and ranking. There are also other preprocessing steps (e.g., standardizing queries, tokenization, spell check) and post-processing steps (e.g., filtering undesirable items, business logic) though we won’t discuss them in this writeup.</p> <p>Candidate retrieval is a fast—but coarse—step to narrow down millions of items into hundreds of candidates. We trade off precision for efficiency to quickly narrow the search space (e.g., from millions to hundreds, a 99.99% reduction) for the downstream ranking task. Most contemporary retrieval methods convert the input (i.e., item, search query) into an embedding before using ANN to find similar items. Nonetheless, in the examples below, we’ll also see systems using graphs (DoorDash) and decision trees (LinkedIn).</p> <p>Ranking is a slower—but more precise—step to score and rank top candidates. As we’re processing fewer items (i.e., hundreds instead of millions), we have room to add features that would have been infeasible in the retrieval step (due to compute and latency constraints). Such features include item and user data, and contextual information. We can also use more sophisticated models with more layers and parameters.</p> <p>In the offline environment, data flows bottom-up, where we use training data and item/user data to create artifacts such as models, ANN indices, and feature stores. These artifacts are then loaded into the online environment (via the dashed arrows). In the online environment, each request flows left to right, through the retrieval and ranking steps before returning a set of results (e.g., recommendations, search results).</p>]]></content><author><name></name></author><category term="machine-learning"/><summary type="html"><![CDATA[study notes]]></summary></entry><entry><title type="html">Math for ML</title><link href="https://michaelc-yu.github.io/blog/2024/math/" rel="alternate" type="text/html" title="Math for ML"/><published>2024-11-24T13:36:11+00:00</published><updated>2024-11-24T13:36:11+00:00</updated><id>https://michaelc-yu.github.io/blog/2024/math</id><content type="html" xml:base="https://michaelc-yu.github.io/blog/2024/math/"><![CDATA[<h4 id="vector-calculus">Vector Calculus</h4> <p>Taylor Polynomial is an approximation of a function f(x) around a point \(x_0\) using a polynomial constructed from the derivatives of f(x) at that point.</p> <p>Def: The Taylor Polynomial of degree n of \(f: \mathbb{R} \rightarrow \mathbb{R}\) at \(x_0\) is defined as <br/> \(T_n(x) = \sum_{k=0}^n \frac{f^{(k)}(x_0)}{k!} (x-x_0)^k\) <br/><br/> where \(f^{(k)}(x_0)\) is the \(k^{\text{th}}\) derivative of f at \(x_0\) and \(\frac{f^{(k)}(x_0)}{k!}\) are the coefficients of the polynomial.</p> <p><br/></p> <p><strong>Functions</strong> <br/> Product rule: \((f(x) g(x))' = f'(x) g(x) + f(x) g'(x)\) <br/> Quotient rule: \((f(x) / g(x))' = (f'(x) g(x) - f(x) g'(x)) / (g(x))^2\) <br/> Sum rule: \((f(x) + g(x))' = f'(x) + g'(x)\) <br/> Chain rule: \((g(f(x)))' = (g o f)'(x) = g'(f(x))f'(x)\)</p> <p><br/> The generalization of the derivative to functions of several variables is the gradient. We find the gradient of the function \(f\) with respect to x by varying one variable at a time and keeping the others constant.</p> <p>Def: For a function \(f: \mathbb{R}^n \rightarrow \mathbb{R}, x \rightarrow f(x), x \in \mathbb{R}^n\) of n variables \(x_1, ..., x_n\) we define the partial derivatives as</p> <p>\(\frac{df}{dx_1}\) = \(\lim_{h \to 0} \frac{f(x_1 + h, x_2, ..., x_n) - f(x)}{h}\) <br/> … <br/> \(\frac{df}{dx_n}\) = \(\lim_{h \to 0} \frac{f(x_1, x_2, ..., x_n + h) - f(x)}{h}\)</p> <p>When we collect them in a row vector form, this is called the gradient of \(f\) or the <strong>Jacobian</strong>.</p> <p><br/> Sometimes we are interested in derivatives of higher order. Consider a function \(f: \mathbb{R}^2 \rightarrow \mathbb{R}\) of two variables x, y. We use the following notation for higher-order partial derivatives (and for gradients):</p> <p>\(\frac{d^2 f}{dx^2}\) is the second partial derivative of f with respect to x <br/> \(\frac{d^n f}{dx^n}\) is the \(n^{\text{th}}\) partial derivative of f with respect to x <br/></p> <p>The Hessian is the collection of all second-order partial derivatives. It generalizes the second derivative to multiple dimensions and is used to study the curvature of a function. It is a square matrix.</p> <p><br/></p> <p><strong>Multivariate Taylor Series</strong></p> <p>We consider a function \(f: \mathbb{R}^D \rightarrow \mathbb{R}\) \(x \rightarrow f(x), x \in \mathbb{R}^D\)</p> <p>that is smooth at \(x_0\). The multivariate Taylor series of \(f\) at \((x_0)\) is defined as</p> <p>\(f(x) = \sum_{k=0}^\infty \frac{D_x^{k}f(x_0)}{k!} (x-x_0)^k\) <br/><br/> where \({D_x^{k}f(x_0)}\) is the \(k^{\text{th}}\) total derivative of \(f\) with respect to x, evaluated at \(x_0\). <br/><br/> The Taylor series is an infinite series that represents a function as a sum of terms based on its derivatives at a point. The Taylor series provides a precise representation of the function if it converges to the function at all points near the expansion point. <br/> The Taylor polynomial is a finite approximation of the Taylor series, using only the first few terms (up to n). A Taylor polynomial of degree n includes terms up to the \(n^{\text{th}}\) partial derivatives of the function. It provides a local approximation near a given point but becomes less accurate as you move further from that point.</p> <p><br/></p> <h4 id="probability-and-distributions">Probability and Distributions</h4> <p>The sample space Ω is the set of all possible outcomes of the experiment.</p> <p>The event space \(A\) is a subset of the sample space that represents a particular outcome or group ot outcomes we’re interested in.</p> <p>The probability \(P\). With each event, we associate a number \(P(A)\) that measures the probability that the event will occur.</p> <p><br/></p> <p>The marginal probabbility that \(X\) takes the value x irrespective of the value of random variable \(Y\) is written as p(x).</p> <p>The fraction of instances (the conditional probability) for which \(Y\) = y is written as \(p(y \mid x)\).</p> <p>Def: The <strong>product rule</strong> relates the joint distribution to the conditional distribution via p(x, y) = \(p(y \mid x)\) p(x)</p> <p><strong>Bayes’ theorem</strong> <br/> \(p(x \mid y)\) = \(\frac {p(y \mid x) p(x)}{p(y)}\)</p> <p><br/></p> <p><strong>Expectation</strong></p> <p>\(\mathbb{E}[X] = \sum_{i} x_i P(X=x_i)\) (for discrete variables)</p> <p>\(\mathbb{E}[X] = \int_{-\infty}^\infty x f_x(x) dx\) (for continuous variables)</p> <p>The expected value of a function g of a discrete random variable x is given by</p> \[\mathbb{E}[g(x)] = \sum_{x \in X} g(x) p(x)\] <p>The expected value of a function \(g: \mathbb{R} \rightarrow \mathbb{R}\) of a univariate continuous random variable x is given by</p> \[\mathbb{E}[g(x)] = \int_{x} g(x) p(x) dx\] <p><strong>Sums and Transformations of Random Variables</strong></p> \[\mathbb{E}[x+y] = \mathbb{E}[x] + \mathbb{E}[y]\] \[\mathbb{E}[x-y] = \mathbb{E}[x] - \mathbb{E}[y]\] <p><strong>Covariance</strong></p> <p>The covariance intuitively represents the notion of how dependent random variables are to one another.</p> <p>The covariance bewteen two univariate random variables X, Y \(\in \mathbb{R}\) is given by the expected product of their derivatives from their respective means,</p> \[Cov_{\text{x,y}} [x, y] = \mathbb{E}_{\text{x,y}} [(x-\mathbb{E}_x[x]) (y-\mathbb{E}_y[y])]\] <p>The covariance of a variable with itself is called the variance and is denoted by \(V_x[x]\). The square root of the variance is called the standard deviation \(\sigma(x)\)</p> <p>If we consider two multivariate random variables X and Y with states \(x \in \mathbb{R}^D\) and \(y \in \mathbb{R}^E\) respectively, the covariance between X and Y is defined as</p> <p>Cov[x, y] = \(\mathbb{E}[xy^T] - \mathbb{E}[x]\mathbb{E}[y]^T = Cov[y, x]^T \in \mathbb{R}^{\text{D x E}}\)</p> <p><strong>Correlation</strong></p> <p>The correlation between two random variables X, Y is given by</p> \[Corr[x, y] = \frac{Cov[x,y]}{\sqrt{V[x] V[y]}} \in [-1, 1]\] <p>The covariance (and correlation) indicate how two random variables are related. Positive correlation means that when x grows, then y is also expected to grow. Negative correlation means that as x increases, then y decreases.</p> <p><strong>Statistical Independence</strong></p> <p>Two random variables X,Y are statistically independent if and only if p(x,y) = p(x) p(y)</p> <p><strong>Gaussian Distribution</strong></p> <p>The Gaussian distribution is one of the most well-studied probability distributions for continuous-valued random variables. It’s also known as the normal distribution. For a univariate random variable, the Gaussian distribution has a density that is given by:</p> \[p(x \mid \mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x - \mu)^2}{2\sigma^2}}\] <h4 id="continuous-optimization">Continuous Optimization</h4> <p><strong>Gradient Descent</strong></p> <p>If we want to find a local optimum \(f(x_0)\) of a function \(f: \mathbb{R}^n \rightarrow \mathbb{R}, x \rightarrow f(x)\), we start with an initial guess \(x_0\) of the parameters we wish to optimize and then iterate according to</p> \[x_{\text{i+1}} = x_i - y_i ((\nabla{f}) (x_i))^T\] <p>For a suitable step-size \(y_i\), the sequence \(f(x_0) \geq f(x_1) \geq ...\) converges to a local minimum.</p> <p><strong>Convex Optimization</strong></p> <p>A <strong>convex function</strong> is a type of function with a specific shape and property that is crucial in optimization. Intuitively, a convex function has a bowl-shaped or U-shaped curve, which makes it easier to find the minimum of the function because any local minimum is also a global minimum.</p> <p>A set C is a <strong>convex set</strong> if for any \(x, y \in C\) and for any scalar \(\theta\) with \(0 \leq \theta \leq 1\), we have</p> \[\theta x + (1 - \theta) y \in C\] <p>Convex sets are sets such that a straight line connecting any two elements of the set lie inside the set.</p> <p><strong>Convex functions</strong> are functions such that a straight line between any two points of the function lie above the function.</p>]]></content><author><name></name></author><category term="mathematics"/><summary type="html"><![CDATA[notes from Mathematics for Machine Learning by Deisenroth, Faisal, Ong]]></summary></entry><entry><title type="html">Neurobiology</title><link href="https://michaelc-yu.github.io/blog/2024/neuroscience/" rel="alternate" type="text/html" title="Neurobiology"/><published>2024-10-26T23:36:10+00:00</published><updated>2024-10-26T23:36:10+00:00</updated><id>https://michaelc-yu.github.io/blog/2024/neuroscience</id><content type="html" xml:base="https://michaelc-yu.github.io/blog/2024/neuroscience/"><![CDATA[<hr/> <h3 id="neural-networks-in-the-brain">Neural Networks in the Brain</h3> <p><br/></p> <p>Spreading-activation model: proposes that the activation of any one concept initiates a spread of activity to nearby concepts in the network, which primes those concepts so they become temporarily more retrievable than they were before.</p> <ul> <li>ex. Sky, ocean, blueberries, sapphires all linked to “blue”</li> </ul> <p>Modern brain theories posit that memories for concepts are stored in overlapping neural circuits in the cerebral cortex. And thus priming occurs because the activation of the circuit for one concept activates the part of the circuit for another.</p> <p>Retrieval of one memory causes activation to spread out to the nearby nodes representing related memories. In neurobiology we think of neurons as being arranged in networks. The more distant the connection between neurons, the longer it should take for one neuron to activate another, and the less likely it would be for that one neuron to activate the other.</p> <figure> <picture> <img src="/assets/img/spreading.jpg" width="50%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><br/></p> <p>In other words, information is not coded in a single molecule, synapse, or neuron. Information is coded in networks and patterns of neuronal activity.</p> <p>Creativity can be thought of as networks that spread far wider, and making connections that neurons in another individual does not.</p> <hr/> <h3 id="long-term-potentiation">Long-Term Potentiation</h3> <p><br/></p> <p>Long-term potentiation (LTP) is a process involving persistent strengthening of synapses that leads to a long-lasting increase in signal transmission between neurons. It is an important process in the context of synaptic plasticity. LTP recording is widely recognized as a cellular model for the study of memory.</p> <p>LTP is like a marriage bond, connections are stronger between the presynpatic and postsynaptic neurons.</p> <p>Long Term Potentiation - the biomolecular process that your neurons go through as you learn</p> <ul> <li>how connections between neurons are strengthened through repeated pairing/firing</li> <li>neurons that fire together, wire together (using LTP)</li> </ul> <p>Our brain cells communicate with one another via synaptic transmission - one brain cell releases a chemical (neurotransmitter) that the next brain cell absorbs. This communication process is known as “neuronal firing.” When brain cells communicate frequently, the connection between them strengthens. Messages that travel the same pathway in the brain over &amp; over begin to transmit faster and faster. With enough repetition, they become automatic. That’s why we practice things like hitting a golf ball - with enough practice, we can go on automatic pilot. <br/><br/><br/> A dendritic spine is a small membranous profusion from a neuron’s dendrite that typically receives input from a single axon at the synapse.</p> <p>The more the spines are stimulated, the bigger, more stable, and longer lived they become (in contrast with smaller, immature spines, which are more transient). These large, so-called mushroom or stubby spines are thought to be one of the physical correlates of memory.</p> <figure> <picture> <img src="/assets/img/spine.jpg" width="60%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Repeated long-term potentiation causes spines to become larger and heartier. <br/><br/></p> <p>Synapses are strengthened and maintained in stages: Changes in synaptic strength that support LTP evolve in stages that can be identified by the unique molecular processes that support each stage.</p> <p>Lasting synaptic changes depend on a temporally ordered sequence of molecular events</p> <ul> <li>Generation, Stabilization, Consolidation, Maintenance</li> </ul> <p><br/></p> <p>To create permanent memories through this system, emphasize occasional activation of neural pathways. Occasional activation signals a host of maintenance molecular processes.</p>]]></content><author><name></name></author><category term="neurobiology"/><summary type="html"><![CDATA[Neural Networks in the Brain]]></summary></entry></feed>