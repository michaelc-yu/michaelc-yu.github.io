---
layout: post
title: math formulas
date: 2024-11-24 13:36:11
description: notes from Mathematics for Machine Learning by Deisenroth, Faisal, Ong
tags: mathematics
featured: false
related_posts: false
---

#### Vector Calculus

Taylor Polynomial is an approximation of a function f(x) around a point $$x_0$$ using a polynomial constructed from the derivatives of f(x) at that point.

Def: The Taylor Polynomial of degree n of $$f: \mathbb{R} \rightarrow \mathbb{R}$$ at $$x_0$$ is defined as
<br>
$$
T_n(x) = \sum_{k=0}^n \frac{f^{(k)}(x_0)}{k!} (x-x_0)^k
$$
<br><br>
where $$f^{(k)}(x_0)$$ is the $$k^{\text{th}}$$ derivative of f at $$x_0$$ and $$\frac{f^{(k)}(x_0)}{k!}$$ are the coefficients of the polynomial.

<br>

**Functions** 
<br>
Product rule: $$ (f(x) g(x))' = f'(x) g(x) + f(x) g'(x) $$
<br>
Quotient rule: $$ (f(x) / g(x))' = (f'(x) g(x) - f(x) g'(x)) / (g(x))^2 $$
<br>
Sum rule: $$ (f(x) + g(x))' = f'(x) + g'(x) $$
<br>
Chain rule: $$ (g(f(x)))' = (g o f)'(x) = g'(f(x))f'(x) $$

<br>
The generalization of the derivative to functions of several variables is the gradient.
We find the gradient of the function $$f$$ with respect to x by varying one variable at a time and keeping the others constant.

Def: For a function  $$ f: \mathbb{R}^n \rightarrow \mathbb{R}, x \rightarrow f(x), x \in \mathbb{R}^n $$ of n variables $$ x_1, ..., x_n $$ we define the partial derivatives as

$$ \frac{df}{dx_1} $$ =  $$ \lim_{h \to 0} \frac{f(x_1 + h, x_2, ..., x_n) - f(x)}{h}$$ 
<br>
...
<br>
$$ \frac{df}{dx_n} $$ =  $$ \lim_{h \to 0} \frac{f(x_1, x_2, ..., x_n + h) - f(x)}{h}$$ 

When we collect them in a row vector form, this is called the gradient of $$f$$ or the Jacobian.

<br>
Sometimes we are interested in derivatives of higher order.
Consider a function $$ f: \mathbb{R}^2 \rightarrow \mathbb{R} $$ of two variables x, y. We use the following notation for higher-order partial derivatives (and for gradients):

$$ \frac{d^2 f}{dx^2} $$ is the second partial derivative of f with respect to x
<br>
$$ \frac{d^n f}{dx^n} $$ is the $$n^{\text{th}}$$ partial derivative of f with respect to x
<br>

The Hessian is the collection of all second-order partial derivatives. It generalizes the second derivative to multiple dimensions and is used to study the curvature of a function. It is a square matrix.

<br>

**Multivariate Taylor Series**

We consider a function
$$ f: \mathbb{R}^D \rightarrow \mathbb{R} $$
$$ x \rightarrow f(x), x \in \mathbb{R}^D $$

that is smooth at $$x_0$$. The multivariate Taylor series of $$f$$ at $$(x_0)$$ is defined as

$$
f(x) = \sum_{k=0}^\infty \frac{D_x^{k}f(x_0)}{k!} (x-x_0)^k
$$
<br><br>
where $${D_x^{k}f(x_0)}$$ is the $$k^{\text{th}}$$ total derivative of $$f$$ with respect to x, evaluated at x_0.
<br><br>
The Taylor series is an infinite series that represents a function as a sum of terms based on its derivatives at a point. The Taylor series provides a precise representation of the function if it converges to the function at all points near the expansion point.
<br>
The Taylor polynomial is a finite approximation of the Taylor series, using only the first few terms (up to n). A Taylor polynomial of degree n includes terms up to the $$n^{\text{th}}$$ partial derivatives of the function. It provides a local approximation near a given point but becomes less accurate as you move further from that point.

<br>

#### Probability and Distributions

The sample space Î© is the set of all possible outcomes of the experiment.

The event space $$A$$ is a subset of the sample space that represents a particular outcome or group ot outcomes we're interested in.

The probability $$P$$. With each event, we associate a number $$P(A)$$ that measures the probability that the event will occur.

<br>

The marginal probabbility that $$X$$ takes the value x irrespective of the value of random variable $$Y$$ is written as p(x).

The fraction of instances (the conditional probability) for which $$Y$$ = y is written as $$p(y \mid x)$$.

Def: The **product rule** relates the joint distribution to the conditional distribution via
p(x, y) = $$p(y \mid x)$$ p(x)

**Bayes' theorem**
<br>
$$ p(x \mid y) $$ = $$\frac {p(y \mid x) p(x)}{p(y)} $$

<br>

**Expectation**

$$ \mathbb{E}[X] = \sum_{i} x_i P(X=x_i) $$  (for discrete variables)


$$ \mathbb{E}[X] = \int_{-\infty}^\infty x f_x(x) dx $$  (for continuous variables)

The expected value of a function g of a discrete random variable x is given by

$$ \mathbb{E}[g(x)] = \sum_{x \in X} g(x) p(x) $$

The expected value of a function $$ g: \mathbb{R} \rightarrow \mathbb{R} $$ of a univariate continuous random variable x is given by

$$ \mathbb{E}[g(x)] = \int_{x} g(x) p(x) dx $$


**Sums and Transformations of Random Variables**

$$ \mathbb{E}[x+y] = \mathbb{E}[x] + \mathbb{E}[y] $$

$$ \mathbb{E}[x-y] = \mathbb{E}[x] - \mathbb{E}[y] $$


**Statistical Independence**

Two random variables X,Y are statistically independent if and only if
p(x,y) = p(x) p(y)


**Gaussian Distribution**

The Gaussian distribution is one of the most well-studied probability distributions for continuous-valued random variables. It's also known as the normal distribution. For a univariate random variable, the Gaussian distribution has a density that is given by:

$$ p(x \mid \mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x - \mu)^2}{2\sigma^2}} $$









